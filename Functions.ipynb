{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains all the functions implemented through the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Tuple, List, Dict, Union, Optional, Any, Generator\n",
    "\n",
    "import random\n",
    "\n",
    "from scipy.stats import rv_continuous, kstest, norm\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "from datetime import timedelta\n",
    "from pandas import Timestamp\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Financial Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaWeights(cov: np.ndarray, riskDist: np.ndarray = None,\n",
    "               riskTarget: float = 1.) -> np.ndarray:\n",
    "    eVal, eVec = np.linalg.eigh(cov)\n",
    "    indices = eVal.argsort()[::-1]\n",
    "    eVal, eVec = eVal[indices], eVec[:, indices]    # sorting by decreasing eVal (i.e. decreasing variance)\n",
    "    if riskDist is None:\n",
    "        riskDist = np.zeros(cov.shape[0])\n",
    "        riskdist[-1] = 1.\n",
    "    loads = riskTarget * (riskDist / eVal) ** 0.5\n",
    "    weights = np.dot(eVec, np.reshape(loads, (-1, 1)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical CUSUM filter\n",
    "def getTEvents(gRaw: pd.Series, h: float) -> np.ndarray:\n",
    "    gRaw = gRaw[~gRaw.index.duplicated(keep='first')]\n",
    "    tEvents, sPos, sNeg = [], 0, 0\n",
    "    diff = gRaw.diff()\n",
    "    for i in diff.index[1:]:\n",
    "        sPos, sNeg = max(0, sPos + diff.loc[i]), min(0, sNeg + diff.loc[i])\n",
    "        if sNeg < -h:\n",
    "            sNeg = 0\n",
    "            tEvents.append(i)\n",
    "        elif sPos > h:\n",
    "            sPos = 0\n",
    "            tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5\n",
    "def get_tick_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                  times: np.ndarray, freq: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(range(freq, len(prices), freq)), 6), dtype=object)\n",
    "    ind = 0\n",
    "    for i in range(freq, len(prices), freq):\n",
    "        bars[ind][0] = pd.Timestamp(times[i - 1])          # time\n",
    "        bars[ind][1] = prices[i - freq]                    # open\n",
    "        bars[ind][2] = np.max(prices[i - freq: i])         # high\n",
    "        bars[ind][3] = np.min(prices[i - freq: i])         # low\n",
    "        bars[ind][4] = prices[i - 1]                       # close\n",
    "        bars[ind][5] = np.sum(vols[i - freq: i])           # volume\n",
    "        ind += 1\n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                    times: np.ndarray, bar_vol: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(prices), 6), dtype=object)\n",
    "    ind = 0\n",
    "    last_tick = 0\n",
    "    cur_volume = 0\n",
    "    for i in range(len(prices)):\n",
    "        cur_volume += vols[i]\n",
    "        if cur_volume >= bar_vol:\n",
    "            bars[ind][0] = pd.Timestamp(times[i - 1])            # time\n",
    "            bars[ind][1] = prices[last_tick]                     # open\n",
    "            bars[ind][2] = np.max(prices[last_tick: i + 1])      # high\n",
    "            bars[ind][3] = np.min(prices[last_tick: i + 1])      # low\n",
    "            bars[ind][4] = prices[i]                             # close\n",
    "            bars[ind][5] = np.sum(vols[last_tick: i + 1])        # volume\n",
    "            cur_volume = 0\n",
    "            last_tick = i + 1\n",
    "            ind += 1\n",
    "    return bars[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dollar_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                    times: np.ndarray, bar_sum: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(prices), 6), dtype=object)\n",
    "    ind = 0\n",
    "    last_tick = 0\n",
    "    cur_sum = 0\n",
    "    for i in range(len(prices)):\n",
    "        cur_sum += vols[i] * prices[i]\n",
    "        if cur_sum >= bar_sum:\n",
    "            bars[ind][0] = pd.Timestamp(times[i - 1])            # time\n",
    "            bars[ind][1] = prices[last_tick]                     # open\n",
    "            bars[ind][2] = np.max(prices[last_tick: i + 1])      # high\n",
    "            bars[ind][3] = np.min(prices[last_tick: i + 1])      # low\n",
    "            bars[ind][4] = prices[i]                             # close\n",
    "            bars[ind][5] = np.sum(vols[last_tick: i + 1])        # volume\n",
    "            cur_sum = 0\n",
    "            last_tick = i + 1\n",
    "            ind += 1\n",
    "    return bars[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bollinger_bands(dollar_bars: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    prices = dollar_bars[:, 4]    # taking close prices\n",
    "    ma = (pd.Series(prices).rolling(20, min_periods=20).mean())      # 20 bars moving average\n",
    "    sigma = pd.Series(prices).rolling(20, min_periods=20).std()\n",
    "    b_upper, b_lower = (ma + alpha * sigma), (ma - alpha * sigma)    # bollinger bounds    \n",
    "    return np.array([ma, b_upper, b_lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(bars: np.ndarray) -> np.ndarray:\n",
    "    close_prices = pd.Series(bars[:, 4], index=bars[:, 0])\n",
    "    return (close_prices.diff() / close_prices)[1:, ].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_vol(close: pd.Series, span0: int = 20) -> pd.Series:\n",
    "    df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "    df0 = df0[df0 > 0]\n",
    "    df0 = pd.Series(close.index[df0 - 1], index=close.index[close.shape[0] - df0.shape[0]:])\n",
    "    df0 = close.loc[df0.index] / close.loc[df0.values].values - 1    # daily returns\n",
    "    df0 = df0.ewm(span=span0).std()\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tripple_barrier(close: pd.Series, events: pd.DataFrame,\n",
    "                                   pt_sl: List, molecule: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Labeling observations using tripple-barrier method\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            events (pd.DataFrame): dataframe with columns:\n",
    "                                   - t1: The timestamp of vertical barrier (if np.nan, there will not be\n",
    "                                         a vertical barrier)\n",
    "                                   - trgt: The unit width of the horizontal barriers\n",
    "            pt_sl (list): list of two non-negative float values:\n",
    "                          - pt_sl[0]: The factor that multiplies trgt to set the width of the upper barrier.\n",
    "                                      If 0, there will not be an upper barrier.\n",
    "                          - pt_sl[1]: The factor that multiplies trgt to set the width of the lower barrier.\n",
    "                                      If 0, there will not be a lower barrier.\n",
    "            molecule (np.ndarray):  subset of event indices that will be processed by a\n",
    "                                    single thread (will be used later)\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe with columns [pt, sl, t1] corresponding to timestamps at which\n",
    "                                each barrier was touched (if it happened)\n",
    "    '''\n",
    "    events_ = events.loc[molecule]\n",
    "    out = events_[['t1']].copy(deep=True)\n",
    "    if pt_sl[0] > 0:\n",
    "        pt = pt_sl[0] * events_['trgt']\n",
    "    else:\n",
    "        pt = pd.Series(data=[np.nan] * len(events.index), index=events.index)    # NaNs\n",
    "    if pt_sl[1] > 0:\n",
    "        sl = -pt_sl[1] * events_['trgt']\n",
    "    else:\n",
    "        sl = pd.Series(data=[np.nan] * len(events.index), index=events.index)    # NaNs\n",
    "    \n",
    "    for loc, t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        df0 = close[loc: t1]                                       # path prices\n",
    "        df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']     # path returns\n",
    "        out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()        # earlisest stop loss\n",
    "        out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()        # earlisest profit taking\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including metalabeleing possibility\n",
    "def get_events_tripple_barrier(\n",
    "    close: pd.Series, tEvents: np.ndarray, pt_sl: float, trgt: pd.Series, minRet: float,\n",
    "    numThreads: int = 1, t1: Union[pd.Series, bool] = False, side: pd.Series = None\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Getting times of the first barrier touch\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            tEvents (np.ndarray): np.ndarray of timestamps that seed every barrier (they can be generated\n",
    "                                  by CUSUM filter for example)\n",
    "            pt_sl (float): non-negative float that sets the width of the two barriers (if 0 then no barrier)\n",
    "            trgt (pd.Series): s series of targets expressed in terms of absolute returns\n",
    "            minRet (float): minimum target return required for running a triple barrier search\n",
    "            numThreads (int): number of threads to use concurrently\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers (pass False\n",
    "                            to disable vertical barriers)\n",
    "            side (pd.Series) (optional): metalabels containing sides of bets\n",
    "        \n",
    "        Returns:\n",
    "            events (pd.DataFrame): dataframe with columns:\n",
    "                                       - t1: timestamp of the first barrier touch\n",
    "                                       - trgt: target that was used to generate the horizontal barriers\n",
    "                                       - side (optional): side of bets\n",
    "    '''\n",
    "    trgt = trgt.loc[trgt.index.intersection(tEvents)]\n",
    "    trgt = trgt[trgt > minRet]\n",
    "    if t1 is False:\n",
    "        t1 = pd.Series(pd.NaT, index=tEvents)\n",
    "    if side is None:\n",
    "        side_, pt_sl_ = pd.Series(np.array([1.] * len(trgt.index)), index=trgt.index), [pt_sl[0], pt_sl[0]]\n",
    "    else:\n",
    "        side_, pt_sl_ = side.loc[trgt.index.intersection(side.index)], pt_sl[:2]\n",
    "    events = pd.concat({'t1': t1, 'trgt': trgt, 'side': side_}, axis=1).dropna(subset=['trgt'])\n",
    "    df0 = apply_tripple_barrier(close, events, pt_sl_, events.index)\n",
    "#     df0 = mpPandasObj(func=apply_tripple_barrier, pdObj=('molecule', events.index),\n",
    "#                       numThreads=numThreads, close=close, events=events, pt_sl=[pt_sl, pt_sl])\n",
    "    events['t1'] = df0.dropna(how='all').min(axis=1)\n",
    "    if side is None:\n",
    "        events = events.drop('side', axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vertical_barrier(close: pd.Series, tEvents: np.ndarray, numDays: int) -> pd.Series:\n",
    "    t1 = close.index.searchsorted(tEvents + pd.Timedelta(days=numDays))\n",
    "    t1 = t1[t1 < close.shape[0]]\n",
    "    t1 = pd.Series(close.index[t1], index=tEvents[:t1.shape[0]])    # adding NaNs to the end\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including metalabeling possibility & modified to generate 0 labels\n",
    "def get_bins(close: pd.Series, events: pd.DataFrame, t1: Union[pd.Series, bool] = False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Generating labels with possibility of knowing the side (metalabeling)\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            events (pd.DataFrame): dataframe returned by 'get_events' with columns:\n",
    "                                   - index: event starttime\n",
    "                                   - t1: event endtime\n",
    "                                   - trgt: event target\n",
    "                                   - side (optional): position side\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers (pass False\n",
    "                            to disable vertical barriers)\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe with columns:\n",
    "                                       - ret: return realized at the time of the first touched barrier\n",
    "                                       - bin: if metalabeling ('side' in events), then {0, 1} (take the bet or pass)\n",
    "                                              if no metalabeling, then {-1, 1} (buy or sell)\n",
    "    '''\n",
    "    events_ = events.dropna(subset=['t1'])\n",
    "    px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "    out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "    if 'side' in events_:\n",
    "        out['ret'] *= events_['side']\n",
    "    out['bin'] = np.sign(out['ret'])\n",
    "    if 'side' in events_:\n",
    "        out.loc[out['ret'] <= 0, 'bin'] = 0\n",
    "    else:\n",
    "        if t1 is not None:\n",
    "            vertical_first_touch_idx = events_[events_['t1'].isin(t1.values)].index\n",
    "            out.loc[vertical_first_touch_idx, 'bin'] = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_labels(labels: pd.DataFrame, min_pct: float = 0.05) -> pd.DataFrame:\n",
    "    while True:\n",
    "        df0 = labels['bin'].value_counts(normalize=True)\n",
    "        if df0.min() > min_pct or df0.shape[0] < 3:\n",
    "            break\n",
    "        print('dropped label', df0.argmin(), df0.min())\n",
    "        labels = labels[labels['bin'] != df0.index[df0.argmin()]]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bollinger_bands_df(dollar_bars: pd.DataFrame, alpha: float) -> np.ndarray:\n",
    "    prices = dollar_bars['close']                                    # taking close prices\n",
    "    ma = (prices.rolling(30, min_periods=1).mean())                  # 30 bars moving average\n",
    "    sigma = prices.rolling(30, min_periods=1).std()\n",
    "    sigma[0] = 0\n",
    "    b_upper, b_lower = (ma + alpha * sigma), (ma - alpha * sigma)    # bollinger bounds    \n",
    "    return np.array([ma, b_upper, b_lower])\n",
    "\n",
    "def get_upside_bars_bb(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df[(df['open'] < df['b_upper']) & (df['close'] > df['b_upper'])]\n",
    "\n",
    "def get_downside_bars_bb(df: pd.DataFrame) -> np.ndarray:\n",
    "    return df[(df['open'] > df['b_lower']) & (df['close'] < df['b_lower'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_conc_events(closeIdx: np.ndarray, t1: pd.Series, molecule: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Computing the number of concurrent events per bar\n",
    "    \n",
    "        Parameters:\n",
    "            closeIdx (np.ndarray): timestamps of close prices\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series with number of labels concurrent at each timestamp\n",
    "    '''\n",
    "    t1 = t1.fillna(closeIdx[-1])\n",
    "    t1 = t1[t1 >= molecule[0]]\n",
    "    t1 = t1.loc[:t1[molecule].max()]\n",
    "    iloc = closeIdx.searchsorted(pd.DatetimeIndex([t1.index[0], t1.max()]))\n",
    "    count = pd.Series([0] * (iloc[1] + 1 - iloc[0]), index=closeIdx[iloc[0]: iloc[1] + 1])\n",
    "    for tIn, tOut in t1.iteritems():\n",
    "        count.loc[tIn: tOut] += 1\n",
    "    return count.loc[molecule[0]: t1[molecule].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weights(t1: pd.Series, num_conc_events: pd.Series, molecule: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Computing average uniqueness over the event's lifespan\n",
    "    \n",
    "        Parameters:\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            num_conc_events (pd.Series): number of concurrent events per bar\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            weights (pd.Series): weights that represent the average uniqueness\n",
    "    '''\n",
    "    weights = pd.Series([0] * len(molecule), index=molecule)\n",
    "    for tIn, tOut in t1.loc[weights.index].iteritems():\n",
    "        weights.loc[tIn] = (1.0 / num_conc_events.loc[tIn: tOut]).mean()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_matrix(barIdx: np.ndarray, t1: pd.Series) -> pd.DataFrame:\n",
    "    '''\n",
    "    Deriving indicator matrix\n",
    "    \n",
    "        Parameters:\n",
    "            barIdx (np.ndarray): indexes of bars\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            \n",
    "        Returns:\n",
    "            indM (pd.DataFrame): binary matrix indicating what bars influence the label for each observation\n",
    "    '''\n",
    "    indM = pd.DataFrame(0, index=barIdx, columns=range(t1.shape[0]))\n",
    "    for i, (t0, t1) in enumerate(t1.iteritems()):\n",
    "        indM.loc[t0:t1, i] = 1.0\n",
    "    return indM\n",
    "\n",
    "\n",
    "def get_avg_uniqueness(indM: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Compute average uniqueness from indicator matrix\n",
    "    '''\n",
    "    c = indM.sum(axis=1)\n",
    "    u = indM.div(c, axis=0)\n",
    "    avg_uniq = u[u > 0].mean()\n",
    "    return avg_uniq\n",
    "\n",
    "\n",
    "def seq_bootstrap(indM: pd.DataFrame, sLength: int = None) -> np.ndarray:\n",
    "    '''\n",
    "    Generate a sample via sequential bootstrap\n",
    "    \n",
    "        Parameters:\n",
    "            indM (pd.DataFrame): binary matrix indicating what bars influence the label for each observation\n",
    "            sLength (int) (optional): sample length (if None, equals number of columns in indM)\n",
    "            \n",
    "        Returns:\n",
    "            phi (np.ndarray): array with indexes of the features sampled by sequential bootstrap\n",
    "    '''\n",
    "    if sLength is None:\n",
    "        sLength = indM.shape[1]\n",
    "    phi = []\n",
    "    while len(phi) < sLength:\n",
    "        avg_uniq = pd.Series()\n",
    "        for i in indM:\n",
    "            indM_ = indM[phi + [i]]\n",
    "            avg_uniq.loc[i] = get_avg_uniqueness(indM_).iloc[-1]\n",
    "        prob = avg_uniq / avg_uniq.sum()\n",
    "        phi += [np.random.choice(indM.columns, p=prob)]\n",
    "    return np.array(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_t1(numObs: int, numBars: int, maxH: int) -> pd.Series:\n",
    "    '''\n",
    "    Generate random t1 series\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "        Returns:\n",
    "            t1 (pd.Series)\n",
    "    '''\n",
    "    t1 = pd.Series()\n",
    "    for i in range(numObs):\n",
    "        idx = np.random.randint(0, numBars)\n",
    "        val = idx + np.random.randint(1, maxH)\n",
    "        t1.loc[idx] = val\n",
    "    return t1.sort_index()\n",
    "\n",
    "\n",
    "def aux_MC(numObs: int, numBars: int, maxH: int) -> dict:\n",
    "    '''\n",
    "    Generate random t1 series\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "        Returns:\n",
    "            dict with average uniqueness derived by standard and sequential bootstrap algorithms\n",
    "    '''\n",
    "    t1 = gen_rand_t1(numObs, numBars, maxH)\n",
    "    barIdx = range(t1.max() + 1)\n",
    "    indM = get_ind_matrix(barIdx, t1)\n",
    "    phi = np.random.choice(indM.columns, size=indM.shape[1])\n",
    "    stdU = get_avg_uniqueness(indM[phi]).mean()\n",
    "    phi = seq_bootstrap(indM)\n",
    "    seqU = get_avg_uniqueness(indM[phi]).mean()\n",
    "    return {'stdU': stdU, 'seqU': seqU}\n",
    "\n",
    "\n",
    "def main_MC(numObs: int, numBars: int, maxH: int, numIters: int) -> None:\n",
    "    '''\n",
    "    Run MC simulation for comparing standard and sequential bootstraps\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "            numIters (int): number of MC iterations\n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe containing uniqueness obtained by standard and sequential bootstraps\n",
    "    '''\n",
    "    out = pd.DataFrame()\n",
    "    for i in range(numIters):\n",
    "        out = pd.concat((out, pd.DataFrame([aux_MC(numObs, numBars, maxH)])))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_return_weights(\n",
    "    t1: pd.Series, num_conc_events: pd.Series, close: pd.Series, molecule: np.ndarray\n",
    ") -> pd.Series:\n",
    "    '''\n",
    "     Determination of sample weights by absolute return distribution\n",
    "    \n",
    "        Parameters:\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            num_conc_events (pd.Series): number of concurrent events per bar\n",
    "            close (pd.Series): close prices\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            weights (pd.Series): weights that absolute returns\n",
    "    '''\n",
    "    ret = np.log(close).diff()\n",
    "    weights = pd.Series(index=molecule, dtype=object)\n",
    "    for tIn, tOut in t1.loc[weights.index].iteritems():\n",
    "        weights.loc[tIn] = (ret.loc[tIn: tOut] / num_conc_events.loc[tIn: tOut]).sum()\n",
    "    return weights.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_decay(tW: pd.Series, clfLastW: float = 1.0) -> pd.Series:\n",
    "    '''\n",
    "    Apply piecewise-linear decay to observed uniqueness. Newest observation gets weight=1,\n",
    "    oldest observation gets weight=clfLastW.\n",
    "    \n",
    "        Parameters:\n",
    "            tW (pd.Series): observed uniqueness\n",
    "            clfLastW (float): weight for the oldest observation\n",
    "        \n",
    "        Returns:\n",
    "            clfW (pd.Series): series with time-decay factors\n",
    "    '''\n",
    "    clfW = tW.sort_index().cumsum()\n",
    "    if clfLastW >= 0:\n",
    "        slope = (1.0 - clfLastW) / clfW.iloc[-1]\n",
    "    else:\n",
    "        slope = 1. / ((clfLastW + 1) * clfW.iloc[-1])\n",
    "    const = 1.0 - slope * clfW.iloc[-1]\n",
    "    clfW = const + slope * clfW\n",
    "    clfW[clfW < 0] = 0\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Fractionally Differentiated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(d: float, size: int) -> np.ndarray:\n",
    "    '''\n",
    "    Computing the weights for differentiating the series\n",
    "    \n",
    "        Parameters:\n",
    "            d (float): differentiating factor\n",
    "            size (int): length of weights array\n",
    "            \n",
    "        Returns:\n",
    "            w (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w = [1.0]\n",
    "    for k in range(1, size):\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        w.append(w_)\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w\n",
    "\n",
    "\n",
    "def plot_weights(dRange: list, nPlots: int, size: int) -> None:\n",
    "    '''\n",
    "    Generating plots for weights arrays for different differentiating factors\n",
    "    \n",
    "        Parameters:\n",
    "            dRange (list): list with 2 floats - bounds of the interval\n",
    "            nPlots (int): number of plots\n",
    "            size(int): length of each weights array\n",
    "            \n",
    "        Returns:\n",
    "            weights (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w = pd.DataFrame()\n",
    "    for d in np.linspace(dRange[0], dRange[1], nPlots):\n",
    "        w_ = get_weights(d, size)\n",
    "        w_ = pd.DataFrame(w_, index=range(w_.shape[0])[::-1], columns=[d])\n",
    "        w = w.join(w_, how='outer')\n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    ax.plot(w)\n",
    "    ax.set_xlabel('$k$')\n",
    "    ax.set_ylabel('$w_k$')\n",
    "    ax.legend(np.round(np.linspace(dRange[0], dRange[1], nPlots), 2), loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_diff(series: pd.DataFrame, d: float, thres: float = 0.01) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fractional differentiation with increasing width window\n",
    "    Note 1: For thres=1, nothing is skipped\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]\n",
    "    \n",
    "        Parameters:\n",
    "            series (pd.DataFrame): dataframe with time series\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for skipping some of the first observations\n",
    "        \n",
    "        Returns:\n",
    "            df (pd.DataFrame): dataframe with differentiated series\n",
    "    '''\n",
    "    w = get_weights(d, series.shape[0])\n",
    "    w_ = np.cumsum(abs(w))\n",
    "    w_ /= w_[-1]\n",
    "    skip = w_[w_ > thres].shape[0]\n",
    "    \n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        seriesF, df_ = series[[name]].fillna(method='ffill').dropna(), \\\n",
    "                       pd.Series(index=np.arange(series.shape[0]), dtype=object)\n",
    "        for iloc in range(skip, seriesF.shape[0]):\n",
    "            loc = seriesF.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc, name]):\n",
    "                continue    # exclude NAs\n",
    "            df_[loc] = np.dot(w[-(iloc + 1):, :].T, seriesF.loc[:loc])[0, 0]\n",
    "        df[name] = df_.dropna().copy(deep=True)\n",
    "    df = pd.concat(df, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_ffd(d: float, thres: float) -> np.ndarray:\n",
    "    '''\n",
    "    Computing the weights for differentiating the series with fixed window size\n",
    "    \n",
    "        Parameters:\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for cutting off weights\n",
    "            \n",
    "        Returns:\n",
    "            w (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w, k = [1.0], 1\n",
    "    while True:\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        if abs(w_) < thres:\n",
    "            break\n",
    "        w.append(w_)\n",
    "        k += 1\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_diff_ffd(series: pd.DataFrame, d: float, thres: float = 1e-5) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fractional differentiation with constant width window\n",
    "    Note 1: thres determines the cut-off weight for the window\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]\n",
    "    \n",
    "        Parameters:\n",
    "            series (pd.DataFrame): dataframe with time series\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for cutting off weights\n",
    "        \n",
    "        Returns:\n",
    "            df (pd.DataFrame): dataframe with differentiated series\n",
    "    '''\n",
    "    w = get_weights_ffd(d, thres)\n",
    "    width = len(w) - 1\n",
    "    \n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        seriesF, df_ = series[[name]].fillna(method='ffill').dropna(), \\\n",
    "                       pd.Series(index=np.arange(series.shape[0]), dtype=object)\n",
    "        for iloc1 in range(width, seriesF.shape[0]):\n",
    "            loc0, loc1 = seriesF.index[iloc1 - width], seriesF.index[iloc1]\n",
    "            if not np.isfinite(series.loc[loc1,name]):\n",
    "                continue    # exclude NAs\n",
    "            df_[loc1]=np.dot(w.T,seriesF.loc[loc0:loc1])[0, 0]\n",
    "        df[name] = df_.dropna().copy(deep=True)\n",
    "    df = pd.concat(df, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_min_ffd(process: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "                 apply_constant_width: bool = True, thres: float = 0.01) -> None:\n",
    "    '''\n",
    "    Finding the minimum differentiating factor that passes the ADF test\n",
    "    \n",
    "        Parameters:\n",
    "            process (np.ndarray): array with random process values\n",
    "            apply_constant_width (bool): flag that shows whether to use constant width window (if True)\n",
    "                                         or increasing width window (if False)\n",
    "            thres (float): threshold for cutting off weights\n",
    "    '''\n",
    "    out = pd.DataFrame(columns=['adfStat', 'pVal', 'lags', 'nObs', '95% conf'], dtype=object)\n",
    "    printed = False\n",
    "    \n",
    "    for d in np.linspace(0, 2, 21):\n",
    "        if apply_constant_width:\n",
    "            process_diff = frac_diff_ffd(pd.DataFrame(process), d, thres)\n",
    "        else:\n",
    "            process_diff = frac_diff(pd.DataFrame(process), d, thres)    \n",
    "        test_results = adfuller(process_diff, maxlag=1, regression='c', autolag=None)\n",
    "        out.loc[d] = list(test_results[:4]) + [test_results[4]['5%']]\n",
    "        if test_results[1] <= 0.05 and not printed:\n",
    "            print(f'Minimum d required: {d}')\n",
    "            printed = True\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    ax.plot(out['adfStat'])\n",
    "    ax.axhline(out['95% conf'].mean(), linewidth=1, color='r', linestyle='dotted')\n",
    "    ax.set_title('Searching for minimum $d$')\n",
    "    ax.set_xlabel('$d$')\n",
    "    ax.set_ylabel('ADF statistics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_adf_results(process: np.ndarray) -> None:\n",
    "    '''\n",
    "    Printing the results of the Augmented Dickey–Fuller test\n",
    "    '''\n",
    "    adf, p_value, _, _, _ = adfuller(process, maxlag=1, regression='c', autolag=None)\n",
    "    print(f'ADF statistics: {adf}')\n",
    "    print(f'p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Cross-Validation in Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_times(t1: pd.Series, testTimes: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Given test times, find the times of the training observations\n",
    "    \n",
    "        Parameters:\n",
    "            t1 (pd.Series): start timestamps (t1.index) and end timestamps (t1.values) of observations\n",
    "            testTimes (pd.Series): start and end timestamps of testing observations (structure similar to t1)\n",
    "            \n",
    "        Returns:\n",
    "            train (pd.Series): series with purged observations from the training set\n",
    "    '''\n",
    "    train = t1.copy(deep=True)\n",
    "    for start, end in testTimes.iteritems():\n",
    "        df0 = train[(start <= train.index) & (train.index <= end)].index    # train starts within test\n",
    "        df1 = train[(start <= train) & (train <= end)].index                # train ends within test\n",
    "        df2 = train[(train.index <= start) & (end <= train)].index          # train envelops test\n",
    "        train = train.drop(df0.union(df1).union(df2))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embargo_times(times: np.ndarray, pctEmbargo: float = 0.0) -> pd.Series:\n",
    "    '''\n",
    "    Get embargo time for each bar\n",
    "    \n",
    "        Parameters:\n",
    "            times (np.ndarray): timestamps of bars\n",
    "            pctEmbargo (float): share of observations to drop after test\n",
    "            \n",
    "        Returns:\n",
    "            mbrg (pd.Series): series with bar timestamps (mbrg.index) and embargo time for each bar (mbrg.values)\n",
    "    '''\n",
    "    step = int(times.shape[0] * pctEmbargo)\n",
    "    if step == 0:\n",
    "        mbrg = pd.Series(times, index=times)\n",
    "    else:\n",
    "        mbrg = pd.Series(times[step:], index=times[:-step])\n",
    "        mbrg = mbrg.append(pd.Series(times[-1], index=times[-step:]))\n",
    "    return mbrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurgedKFold(_BaseKFold):\n",
    "    '''\n",
    "    Extend KFold class to work with labels that span intervals.\n",
    "    The train is purged of observations overlapping test-label intervals.\n",
    "    Test set is assumed contiguous (shuffle=False), without training samples in between.\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_splits: int = 3, t1: Optional[pd.Series] = None, pctEmbargo: float = 0.0\n",
    "    ) -> None:\n",
    "        if not isinstance(t1, pd.Series):\n",
    "            raise ValueError('Label Through Dates must be a pd.Series')\n",
    "        super(PurgedKFold, self).__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.t1 = t1\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "        \n",
    "    def split(\n",
    "        self, X: pd.DataFrame, y: Optional[pd.Series] = None, groups: Optional[np.ndarray] = None\n",
    "    ) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n",
    "        if (X.index == self.t1.index).sum() != len(self.t1):\n",
    "            raise ValueError('X and ThruDateValues must have the same index')\n",
    "        indices = np.arange(X.shape[0])\n",
    "        mbrg = int(X.shape[0] * self.pctEmbargo)\n",
    "        test_starts = [(i[0], i[-1] + 1) for i in np.array_split(np.arange(X.shape[0]), self.n_splits)]\n",
    "        for i, j in test_starts:\n",
    "            t0 = self.t1.index[i]    # start of test set\n",
    "            test_indices = indices[i: j]\n",
    "            maxT1Idx = self.t1.index.searchsorted(self.t1[test_indices].max())\n",
    "            train_indices = self.t1.index.searchsorted(self.t1[self.t1 <= t0].index)\n",
    "            if maxT1Idx < X.shape[0]:    # right train (with embargo)\n",
    "                train_indices = np.concatenate((train_indices, indices[maxT1Idx + mbrg:]))\n",
    "            yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvScore(\n",
    "    clf: Any, X: pd.DataFrame, y: pd.Series, sample_weight: pd.Series, scoring: str ='neg_log_loss',\n",
    "    t1: Optional[pd.Series] = None, cv: Optional[int] = None,\n",
    "    cvGen: Optional[PurgedKFold] = None, pctEmbargo: Optional[float] = None\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Calculating cross-validation score.\n",
    "    \n",
    "        Parameters:\n",
    "            clf (Any): model we want to fit\n",
    "            X (pd.DataFrame): feature matrix\n",
    "            y (pd.Series): labels\n",
    "            sample_weight (pd.Series): sample weights\n",
    "            scoring (str): score we want to compute\n",
    "            t1 (pd.Series): start timestamps (t1.index) and end timestamps (t1.values) of observations\n",
    "            cv (int): number of splits\n",
    "            cvGen (PurgedKFold): object of PurgedKFold class to make splitting\n",
    "            pctEmbargo (float): share of observations to drop after test\n",
    "            \n",
    "        Returns:\n",
    "            score (np.ndarray): score for each cross-validation split\n",
    "    '''\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method')\n",
    "    if cvGen is None:\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    score = []\n",
    "    for train, test in cvGen.split(X=X):\n",
    "        fit = clf.fit(X=X.iloc[train, :], y=y.iloc[train], sample_weight=sample_weight.iloc[train].values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X.iloc[test, :])\n",
    "            score_ = -log_loss(y.iloc[test], prob, sample_weight=sample_weight.iloc[test].values, labels=clf.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X.iloc[test, :])\n",
    "            score_ = accuracy_score(y.iloc[test], pred, sample_weight=sample_weight.iloc[test].values)\n",
    "        score.append(score_)\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_results(cv: Union[StratifiedKFold, PurgedKFold], clf: Any, X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    '''\n",
    "    Plots ROC curve for each iteration of cross-validation together with the mean curve\n",
    "    and print cv accuracy.\n",
    "    Based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval\n",
    "    '''\n",
    "    for scoring in ['accuracy', 'precision', 'recall']:\n",
    "        score = cross_val_score(estimator=clf, X=X, y=y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        print(f'CV mean {scoring}: {np.mean(score)}')\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        clf.fit(np.array(X)[train], np.array(y)[train])\n",
    "        viz = RocCurveDisplay.from_estimator(clf, np.array(X)[test], np.array(y)[test], name=\"ROC fold {}\".format(i),\n",
    "                                             alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color=\"b\", label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "            lw=2, alpha=0.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=\"grey\", alpha=0.2, label=r\"$\\pm$ 1 std. dev.\")\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDI(fit: Any, featNames: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculates mean feature importances based on MDI.\n",
    "    \n",
    "        Parameters:\n",
    "            fit (Any): classifier (needs to be tree-based, e.g. Random Forest)\n",
    "            featNames (np.ndarray): list with feature names\n",
    "        \n",
    "        Returns:\n",
    "            imp (pd.DataFrame): dataframe with mean and std of importance for each feature\n",
    "    '''\n",
    "    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}\n",
    "    df0 = pd.DataFrame.from_dict(df0, orient='index')\n",
    "    df0.columns = featNames\n",
    "    df0 = df0.replace(0, np.nan)    # because max_features=1\n",
    "    imp = pd.concat({'mean': df0.mean(), 'std': df0.std() * df0.shape[0] ** (-0.5)}, axis=1)\n",
    "    imp /= imp['mean'].sum()\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_imp_MDA(\n",
    "    clf: Any, X: pd.DataFrame, y: pd.Series, cv: int, sample_weight: pd.Series,\n",
    "    t1: pd.Series, pctEmbargo: float, scoring: str = 'neg_log_loss'\n",
    ") -> Tuple[pd.DataFrame, float]:\n",
    "    '''\n",
    "    Calculates mean feature importances based on OOS score reduction\n",
    "    while also fitting and evaluating classifier.\n",
    "    \n",
    "        Parameters:\n",
    "            clf (Any): model we want to fit\n",
    "            X (pd.DataFrame): feature matrix\n",
    "            y (pd.Series): labels\n",
    "            cv (int): number of splits\n",
    "            sample_weight (pd.Series): sample weights\n",
    "            t1 (pd.Series): start timestamps (t1.index) and end timestamps (t1.values) of observations\n",
    "            pctEmbargo (float): share of observations to drop after test\n",
    "            scoring (str): score we want to compute\n",
    "        \n",
    "        Returns:\n",
    "            imp (pd.DataFrame): dataframe with mean and std of importance for each feature\n",
    "            scr0.mean() (float): mean CV score of classifier\n",
    "    '''\n",
    "    if scoring not in ['neg_log_loss', 'accuracy']:\n",
    "        raise Exception('wrong scoring method.')\n",
    "    cvGen = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged cv\n",
    "    scr0, scr1 = pd.Series(), pd.DataFrame(columns=X.columns, dtype=object)\n",
    "    \n",
    "    for i, (train, test) in enumerate(cvGen.split(X=X)):\n",
    "        X0, y0, w0 = X.iloc[train, :], y.iloc[train], sample_weight.iloc[train]\n",
    "        X1, y1, w1 = X.iloc[test, :], y.iloc[test], sample_weight.iloc[test]\n",
    "        fit = clf.fit(X=X0, y=y0, sample_weight=w0.values)\n",
    "        if scoring == 'neg_log_loss':\n",
    "            prob = fit.predict_proba(X1)\n",
    "            scr0.loc[i] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "        else:\n",
    "            pred = fit.predict(X1)\n",
    "            scr0.loc[i] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "        for j in X.columns:\n",
    "            X1_ = X1.copy(deep=True)\n",
    "            np.random.shuffle(X1_[j].values)    # permutation of a single column\n",
    "            if scoring == 'neg_log_loss':\n",
    "                prob = fit.predict_proba(X1_)\n",
    "                scr1.loc[i, j] = -log_loss(y1, prob, sample_weight=w1.values, labels=clf.classes_)\n",
    "            else:\n",
    "                pred = fit.predict(X1_)\n",
    "                scr1.loc[i, j] = accuracy_score(y1, pred, sample_weight=w1.values)\n",
    "    imp = (-scr1).add(scr0, axis=0)\n",
    "    if scoring == 'neg_log_loss':\n",
    "        imp = imp / -scr1\n",
    "    else:\n",
    "        imp = imp / (1.0 - scr1)\n",
    "    imp = pd.concat({'mean': imp.mean(), 'std': imp.std() * imp.shape[0] ** (-0.5)}, axis=1)\n",
    "    return imp, scr0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_feat_imp_SFI(\n",
    "    featNames: np.ndarray, clf: Any, trnsX: pd.DataFrame, cont: pd.DataFrame, scoring: str, cvGen: PurgedKFold\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Calculates mean feature importances based on Single Feature Importance (SFI).\n",
    "    \n",
    "        Parameters:\n",
    "            featNames (np.ndarray): list with feature names\n",
    "            clf (Any): model we want to fit\n",
    "            trnsX (pd.DataFrame): train dataset\n",
    "            cont (pd.DataFrame): dataframe with observation labels and weights\n",
    "            scoring (str): scoring function used for evaluation\n",
    "            cvGen (PurgedKFold): CV generator (purged)\n",
    "        \n",
    "        Returns:\n",
    "            imp (pd.DataFrame): dataframe with mean and std of importance for each feature\n",
    "    '''\n",
    "    imp = pd.DataFrame(columns=['mean', 'std'], dtype=object)\n",
    "    for featName in featNames:\n",
    "        df0 = cvScore(clf, X=trnsX[[featName]], y=cont['bin'], sample_weight=cont['w'], scoring=scoring, cvGen=cvGen)\n",
    "        imp.loc[featName, 'mean'] = df0.mean()\n",
    "        imp.loc[featName, 'std'] = df0.std() * df0.shape[0] ** (-0.5)\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eVec(dot: np.ndarray, varThres: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Calculates eigenvalues and eigenvectors of dot product matrix that explain varThres of its variance.\n",
    "    \n",
    "        Parameters:\n",
    "            dot (np.ndarray): feature matrix\n",
    "            varThres (float): share of variance we want to explain\n",
    "        \n",
    "        Returns:\n",
    "            eVal (np.ndarray): eigenvalues\n",
    "            eVec (np.ndarray): eigenvectors\n",
    "    '''\n",
    "    eVal, eVec = np.linalg.eigh(dot)\n",
    "    idx = eVal.argsort()[::-1]    # arguments for sorting eVal desc\n",
    "    eVal, eVec = eVal[idx], eVec[:, idx]\n",
    "    eVal = pd.Series(eVal, index=['PC_' + str(i + 1) for i in range(eVal.shape[0])])\n",
    "    eVec = pd.DataFrame(eVec, index=dot.index, columns=eVal.index)\n",
    "    eVec = eVec.loc[:, eVal.index]\n",
    "    cumVar = eVal.cumsum() / eVal.sum()\n",
    "    dim = cumVar.values.searchsorted(varThres)\n",
    "    eVal, eVec = eVal.iloc[:dim + 1], eVec.iloc[:, :dim + 1]\n",
    "    return eVal, eVec\n",
    "\n",
    "\n",
    "def ortho_feats(dfX: pd.DataFrame, varThres: float = 0.95) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given a dataframe dfX of features, compute orthogonal features dfP explaining varThres of variance.\n",
    "    \n",
    "        Parameters:\n",
    "            dfX (pd.DataFrame): feature matrix\n",
    "            varThres (float): share of variance we want to explain\n",
    "        \n",
    "        Returns:\n",
    "            dfP (pd.DataFrame): orthogonal features\n",
    "    '''\n",
    "    dfZ = dfX.sub(dfX.mean(), axis=1).div(dfX.std(), axis=1)    # standardize\n",
    "    dot = pd.DataFrame(np.dot(dfZ.T, dfZ), index=dfX.columns, columns=dfX.columns)\n",
    "    eVal, eVec = get_eVec(dot, varThres)\n",
    "    dfP = np.dot(dfZ, eVec)\n",
    "    return pd.DataFrame(dfP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(n_features: int = 40, n_informative: int = 10,\n",
    "                  n_redundant: int = 10, n_samples: int = 10000) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''\n",
    "    Generate a synthetic dataset with given types of features.\n",
    "    \n",
    "        Parameters:\n",
    "            n_features (int): total number of features\n",
    "            n_informative (int): number of informative features\n",
    "            n_redundant (int): number of redundant features (linear combinations of informative features)\n",
    "            n_samples (int): number of observations\n",
    "        \n",
    "        Returns:\n",
    "            trnsX (pd.DataFrame): synthetic dataset\n",
    "            cont (pd.DataFrame): dataframe with labels ('bin'), weights, and t1 timestamps\n",
    "    '''\n",
    "    trnsX, cont = make_classification(n_samples=n_samples, n_features=n_features, n_informative=n_informative,\n",
    "                                      n_redundant=n_redundant, random_state=0, shuffle=False)\n",
    "    trnsX, cont = pd.DataFrame(trnsX), pd.Series(cont).to_frame('bin')\n",
    "    df0 = ['I_' + str(i) for i in range(n_informative)] + ['R_' + str(i) for i in range(n_redundant)]\n",
    "    df0 += ['N_' + str(i) for i in range(n_features - len(df0))]\n",
    "    trnsX.columns = df0\n",
    "    cont['w'] = 1.0 / cont.shape[0]\n",
    "    cont['t1'] = pd.Series(cont.index, index=cont.index)\n",
    "    return trnsX, cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no multiprocessing here\n",
    "def feat_importance(\n",
    "    trnsX: pd.DataFrame, cont: pd.DataFrame, n_estimators: int = 100, cv: int = 10,\n",
    "    max_samples: float = 1.0, pctEmbargo: float = 0.0, scoring: str = 'accuracy',\n",
    "    method: str = 'SFI', min_weight_fraction_leaf: float = 0.0, ensemble: str = 'bagging'\n",
    ") -> Tuple[pd.DataFrame, float, float]:\n",
    "    '''\n",
    "    Calculate feature importance using given method using bagged decision trees.\n",
    "    \n",
    "        Parameters:\n",
    "            ensemble (str): model type (decision trees bagging or random forest)\n",
    "            trnsX (pd.DataFrame): train dataset\n",
    "            cont (pd.DataFrame): dataframe with labels ('bin'), weights, and t1 timestamps\n",
    "            n_estimators (int): number of trees\n",
    "            cv (int): number of CV splits\n",
    "            max_samples (float): share of samples to draw from X to train each base estimator\n",
    "            pctEmbargo (float): share of observations to drop after test (embargo period)\n",
    "            scoring (str): scoring/loss function\n",
    "            method (str): method used to calculate feature importance\n",
    "            min_weight_fraction_leaf (float): minimum fraction of the sum of weights required to be at a leaf node\n",
    "        \n",
    "        Returns:\n",
    "            imp (pd.DataFrame): dataframe with mean and std of importance for each feature\n",
    "            oob (float): out-of-bag classifier score\n",
    "            oos (float): mean CV score\n",
    "    '''\n",
    "    if ensemble == 'bagging':\n",
    "        clf = DecisionTreeClassifier(criterion='entropy', max_features=1, class_weight='balanced',\n",
    "                                     min_weight_fraction_leaf=min_weight_fraction_leaf)\n",
    "        clf = BaggingClassifier(base_estimator=clf, n_estimators=n_estimators, max_features=1.0,\n",
    "                                max_samples=max_samples, oob_score=True, n_jobs=-1)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, criterion='entropy',\n",
    "                                     min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                     max_features=1, oob_score=True, n_jobs=-1, max_samples=max_samples)\n",
    "    fit = clf.fit(X=trnsX, y=cont['bin'], sample_weight=cont['w'].values)\n",
    "    oob = fit.oob_score_\n",
    "    \n",
    "    if method == 'MDI':\n",
    "        imp = feat_imp_MDI(fit, featNames=trnsX.columns)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'],\n",
    "                      pctEmbargo=pctEmbargo, scoring=scoring).mean()\n",
    "        \n",
    "    elif method=='MDA':\n",
    "        imp, oos = feat_imp_MDA(clf, X=trnsX, y=cont['bin'], cv=cv, sample_weight=cont['w'], t1=cont['t1'],\n",
    "                                pctEmbargo=pctEmbargo, scoring=scoring)\n",
    "        \n",
    "    elif method=='SFI':\n",
    "        cvGen = PurgedKFold(n_splits=cv, t1=cont['t1'], pctEmbargo=pctEmbargo)\n",
    "        oos = cvScore(clf, X=trnsX, y=cont['bin'], sample_weight=cont['w'],\n",
    "                      scoring=scoring, cvGen=cvGen).mean()\n",
    "        imp = aux_feat_imp_SFI(featNames=trnsX.columns, clf=clf, trnsX=trnsX, cont=cont,\n",
    "                               scoring=scoring, cvGen=cvGen)\n",
    "    \n",
    "    return imp, oob, oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_importance(\n",
    "    imp: pd.DataFrame, oob: float, oos: float, method: str,\n",
    "    tag: str = 'test_func', simNum: Optional[str] = None\n",
    ") -> None:\n",
    "    '''\n",
    "    Plots mean feature importance bars with std.\n",
    "    \n",
    "        Parameters:\n",
    "            imp (pd.DataFrame): feature importance\n",
    "            oob (float): out-of-bag score\n",
    "            oos (float): mean CV score\n",
    "            method (str): method to calculate feature importance\n",
    "            tag (str): tag for title\n",
    "            simNum (str): reference for simulation parameters\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(10, imp.shape[0] / 5))\n",
    "    imp = imp.sort_values('mean', ascending=True)\n",
    "    ax.barh(y=imp.index, width=imp['mean'], color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor':'r'})\n",
    "    if method=='MDI':\n",
    "        ax.set_xlim(left=0, right=imp.sum(axis=1).max())\n",
    "        ax.axvline(1.0 / imp.shape[0], linewidth=1, color='r', linestyle='dotted')\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for i, j in zip(ax.patches, imp.index):\n",
    "        ax.text(i.get_width() / 2, i.get_y() + i.get_height() / 2, j, ha='center', va='center', color='black')\n",
    "    ax.set_title(f'tag={tag} | simNum={simNum} | oob={str(round(oob, 4))} | oos={str(round(oos,4))}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(n_features: int = 40, n_informative: int = 10, n_redundant: int = 10,\n",
    "              n_estimators: int = 100, n_samples: int = 10000, cv: int = 10) -> pd.DataFrame:\n",
    "    '''\n",
    "    Run 3 methods to calculate feature importance on synthetic dataset and print the results.\n",
    "    \n",
    "        Parameters:\n",
    "            n_features (int): total number of features\n",
    "            n_informative (int): number of informative features\n",
    "            n_redundant (int): number of redundant features (linear combinations of informative features)\n",
    "            n_estimators (int): number of trees\n",
    "            n_samples (int): number of observations\n",
    "            cv (int): number of CV splits\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe with stats on each method\n",
    "    '''\n",
    "    trnsX, cont = get_test_data(n_features, n_informative, n_redundant, n_samples)\n",
    "    dict0 = {'minWLeaf': [0.0], 'scoring': ['accuracy'], 'method': ['MDI', 'MDA', 'SFI'], 'max_samples': [1.0]}\n",
    "    jobs, out = (dict(zip(dict0, i)) for i in product(*dict0.values())), []\n",
    "    \n",
    "    for job in jobs:\n",
    "        job['simNum'] = job['method'] +'_' + job['scoring'] + '_' + '%.2f'%job['minWLeaf'] + \\\n",
    "                        '_' + str(job['max_samples'])\n",
    "        print(job['simNum'])\n",
    "        imp, oob, oos = feat_importance(trnsX=trnsX, cont=cont, n_estimators=n_estimators,\n",
    "                                        cv=cv, max_samples=job['max_samples'], scoring=job['scoring'],\n",
    "                                        method=job['method'])\n",
    "        plot_feat_importance(imp=imp, oob=oob, oos=oos, method=job['method'],\n",
    "                             tag='test_func', simNum=job['simNum'])\n",
    "        df0 = imp[['mean']] / imp['mean'].abs().sum()\n",
    "        df0['type'] = [i[0] for i in df0.index]\n",
    "        df0 = df0.groupby('type')['mean'].sum().to_dict()\n",
    "        df0.update({'oob': oob, 'oos': oos})\n",
    "        df0.update(job)\n",
    "        out.append(df0)\n",
    "    \n",
    "    out = pd.DataFrame(out).sort_values(['method', 'scoring', 'minWLeaf', 'max_samples'])\n",
    "    out = out[['method', 'scoring', 'minWLeaf', 'max_samples', 'I', 'R', 'N', 'oob', 'oos']]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. Hyper-Parameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipeline(Pipeline):\n",
    "    '''\n",
    "    Augmentation of sklearn Pipeline class that allows to pass 'sample_weight' to 'fit' method.\n",
    "    '''\n",
    "    def fit(\n",
    "        self, X: pd.DataFrame, y: pd.Series, sample_weight: Optional[pd.Series] = None, **fit_params\n",
    "    ) -> 'MyPipeline':\n",
    "        if sample_weight is not None:\n",
    "            fit_params[self.steps[-1][0] + '__sample_weight'] = sample_weight\n",
    "        return super().fit(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_hyper_fit_base(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params\n",
    ") -> Any:\n",
    "    '''\n",
    "    Implements purged GridSearchCV with a possibility of fitting bagging of tuned estimator.\n",
    "    \n",
    "        Parameters:\n",
    "            feat (pd.DataFrame): features dataset\n",
    "            lbl (pd.Series): labels\n",
    "            t1 (pd.Series): start timestamps (t1.index) and end timestamps (t1.values) of observations\n",
    "            pipe_clf (Any): classififer to fit\n",
    "            param_grid (Dict[str, list]): dictionary with parameters values\n",
    "            cv (int): number of splits\n",
    "            bagging (list): bagging parameters (used when bagging[1] is not None)\n",
    "            n_jobs (int): number of jobs to run in parallel\n",
    "            pctEmbargo (float): share of observations to drop after train\n",
    "        \n",
    "        Returns:\n",
    "            gs (Any): fitted best estimator found by grid search\n",
    "    '''\n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    gs=GridSearchCV(estimator=pipe_clf ,param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand clf_hyper_fit_base to incorporate option to use randomized grid search\n",
    "def clf_hyper_fit(\n",
    "    feat: pd.DataFrame, lbl: pd.Series, t1: pd.Series, pipe_clf: Any, param_grid: Dict[str, list],\n",
    "    cv: int = 3, bagging: list = [0, None, 1.0], rndSearchIter: int = 0,\n",
    "    n_jobs: int = -1, pctEmbargo: float = 0.0, **fit_params\n",
    ") -> Any:\n",
    "    '''\n",
    "    Implements purged GridSearchCV with a possibility of fitting bagging of tuned estimator.\n",
    "    \n",
    "        Parameters:\n",
    "            feat (pd.DataFrame): features dataset\n",
    "            lbl (pd.Series): labels\n",
    "            t1 (pd.Series): start timestamps (t1.index) and end timestamps (t1.values) of observations\n",
    "            pipe_clf (Any): classififer to fit\n",
    "            param_grid (Dict[str, list]): dictionary with parameters values\n",
    "            cv (int): number of splits\n",
    "            bagging (list): bagging parameters (used when bagging[1] is not None)\n",
    "            rndSearchIter (int): number of iterations to use in randomized GS (if 0 then apply standard GS)\n",
    "            n_jobs (int): number of jobs to run in parallel\n",
    "            pctEmbargo (float): share of observations to drop after train\n",
    "        \n",
    "        Returns:\n",
    "            gs (Any): fitted best estimator found by grid search\n",
    "    '''\n",
    "    if set(lbl.values) == {0, 1}:\n",
    "        scoring='f1'    # f1 for meta-labeling\n",
    "    else:\n",
    "        scoring='neg_log_loss'    # symmetric towards all cases\n",
    "    inner_cv = PurgedKFold(n_splits=cv, t1=t1, pctEmbargo=pctEmbargo)    # purged\n",
    "    \n",
    "    if rndSearchIter == 0:\n",
    "        gs = GridSearchCV(estimator=pipe_clf, param_grid=param_grid, scoring=scoring, cv=inner_cv, n_jobs=n_jobs)\n",
    "    else:\n",
    "        gs = RandomizedSearchCV(estimator=pipe_clf, param_distributions=param_grid, scoring=scoring,\n",
    "                                cv=inner_cv, n_jobs=n_jobs, n_iter=rndSearchIter)\n",
    "    gs = gs.fit(feat, lbl, **fit_params).best_estimator_    # pipeline\n",
    "    \n",
    "    if bagging[1] is not None and bagging[1] > 0:\n",
    "        gs = BaggingClassifier(base_estimator=MyPipeline(gs.steps), n_estimators=int(bagging[0]),\n",
    "                               max_samples=float(bagging[1]), max_features=float(bagging[2]), n_jobs=n_jobs)\n",
    "        gs = gs.fit(feat, lbl, sample_weight=fit_params[gs.base_estimator.steps[-1][0]+'__sample_weight'])\n",
    "        gs = Pipeline([('bag', gs)])\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logUniform_gen(rv_continuous):\n",
    "    '''\n",
    "    Implements generator of log-uniform random variables.\n",
    "    '''\n",
    "    def _cdf(self, x: float) -> float:\n",
    "        return np.log(x / self.a) / np.log(self.b / self.a)\n",
    "\n",
    "\n",
    "def log_uniform(a: float = 1.0, b: float = np.exp(1.0)) -> 'logUniform_gen':\n",
    "    return logUniform_gen(a=a, b=b, name='log_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IS_sharpe_ratio(clf: Any) -> float:\n",
    "    '''\n",
    "    Given a fitted gridsearch classifier, returns Sharpe ratio of the best estimator's in-sample forecasts.\n",
    "    '''\n",
    "    best_estimator_ind = np.argmin(clf.cv_results_['rank_test_score'])\n",
    "    mean_score = clf.cv_results_['mean_test_score'][best_estimator_ind]\n",
    "    std_score = clf.cv_results_['std_test_score'][best_estimator_ind]\n",
    "    if mean_score < 0:\n",
    "        return -mean_score / std_score\n",
    "    else:\n",
    "        return mean_score / std_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10. Bet Sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_active_signals_(signals: pd.DataFrame, molecule: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Auxilary function for averaging signals. At time loc, averages signal among those still active.\n",
    "    Signal is active if:\n",
    "        a) issued before or at loc AND\n",
    "        b) loc before signal's endtime, or endtime is still unknown (NaT).\n",
    "    \n",
    "        Parameters:\n",
    "            signals (pd.DataFrame): dataset with signals and t1\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.Series): series with average signals for each timestamp\n",
    "    '''\n",
    "    out = pd.Series()\n",
    "    for loc in molecule:\n",
    "        df0 = (signals.index.values <= loc) & ((loc < signals['t1']) | pd.isnull(signals['t1']))\n",
    "        act = signals[df0].index\n",
    "        if len(act) > 0:\n",
    "            out[loc] = signals.loc[act, 'signal'].mean()\n",
    "        else:\n",
    "            out[loc] = 0    # no signals active at this time\n",
    "    return out\n",
    "            \n",
    "\n",
    "def avg_active_signals(signals: pd.DataFrame) -> pd.Series:\n",
    "    '''\n",
    "    Computes the average signal among those active.\n",
    "    \n",
    "        Parameters:\n",
    "            signals (pd.DataFrame): dataset with signals and t1\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.Series): series with average signals for each timestamp\n",
    "    '''\n",
    "    tPnts = set(signals['t1'].dropna().values)\n",
    "    tPnts = tPnts.union(signals.index.values)\n",
    "    tPnts = sorted(list(tPnts))\n",
    "    out = avg_active_signals_(signals=signals, molecule=tPnts)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_signal(signal0: pd.Series, stepSize: float) -> pd.Series:\n",
    "    '''\n",
    "    Discretizes signals.\n",
    "    \n",
    "        Parameters:\n",
    "            signal0 (pd.Series): series with signals\n",
    "            stepSize (float): degree of discretization (must be in (0, 1])\n",
    "        \n",
    "        Returns:\n",
    "            signal1 (pd.Series): series with discretized signals\n",
    "    '''\n",
    "    signal1 = (signal0 / stepSize).round() * stepSize    # discretize\n",
    "    signal1[signal1 > 1] = 1    # cap\n",
    "    signal1[signal1 < -1] = -1    # floor\n",
    "    return signal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no multithreading\n",
    "def get_signal(\n",
    "    events: pd.DataFrame, stepSize: float, prob: pd.Series, pred: pd.Series, numClasses: int, **kargs\n",
    ") -> pd.Series:\n",
    "    '''\n",
    "    Gets signals from predictions. Includes averaging of active bets as well as discretizing final value.\n",
    "    \n",
    "        Parameters:\n",
    "            events (pd.DataFrame): dataframe with columns:\n",
    "                                       - t1: timestamp of the first barrier touch\n",
    "                                       - trgt: target that was used to generate the horizontal barriers\n",
    "                                       - side (optional): side of bets\n",
    "            stepSize (float): ---\n",
    "            prob (pd.Series): series with probabilities of given predictions\n",
    "            pred (pd.Series): series with predictions\n",
    "            numClasses (int): number of classes\n",
    "        \n",
    "        Returns:\n",
    "            signal1 (pd.Series): series with discretized signals\n",
    "    '''\n",
    "    if prob.shape[0] == 0:\n",
    "        return pd.Series()\n",
    "    signal0 = (prob - 1.0 / numClasses) / (prob * (1.0 - prob)) ** 0.5    # t-value\n",
    "    signal0 = pred * (2 * norm.cdf(signal0) - 1)    # signal = side * size\n",
    "    if 'side' in events:\n",
    "        signal0 *= events.loc[signal0.index, 'side']    # meta-labeling\n",
    "    df0 = signal0.to_frame('signal').join(events[['t1']], how='left')\n",
    "    df0 = avg_active_signals(df0)\n",
    "    signal1 = discrete_signal(signal0=df0, stepSize=stepSize)\n",
    "    return signal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bet_size(x: float, w: float) -> float:\n",
    "    '''\n",
    "    Returns bet size given price divergence and sigmoid function coefficient.\n",
    "    \n",
    "        Parameters:\n",
    "            x (float): difference between forecast price and current price f_i - p_t\n",
    "            w (float): coefficient that regulates the width of the sigmoid function\n",
    "        \n",
    "        Returns:\n",
    "            (float): bet size\n",
    "    '''\n",
    "    return x * (w + x ** 2) ** (-0.5)\n",
    "\n",
    "\n",
    "def get_target_pos(w: float, f: float, mP: float, maxPos: float) -> float:\n",
    "    '''\n",
    "    Calculates target position size associated with forecast f.\n",
    "    \n",
    "        Parameters:\n",
    "            w (float): coefficient that regulates the width of the sigmoid function\n",
    "            f (float): forecast price\n",
    "            mP (float): current market price\n",
    "            maxPos (float): maximum absolute position size\n",
    "        \n",
    "        Returns:\n",
    "            (float): target position size\n",
    "    '''\n",
    "    return int(bet_size(w, f - mP) * maxPos)\n",
    "\n",
    "\n",
    "def inv_price(f: float, w: float, m: float) -> float:\n",
    "    '''\n",
    "    Calculates inverse function of bet size with respect to market price p_t.\n",
    "    \n",
    "        Parameters:\n",
    "            f (float): forecast price\n",
    "            w (float): coefficient that regulates the width of the sigmoid function\n",
    "            m (float): bet size\n",
    "            \n",
    "        Returns:\n",
    "            (float): inverse price function\n",
    "    '''\n",
    "    return f - m * (w / (1 - m**2)) ** 0.5\n",
    "\n",
    "\n",
    "def limit_price(tPos: float, pos: float, f: float, w: float, maxPos: float) -> float:\n",
    "    '''\n",
    "    Calculates breakeven limit price p̄ for the order size q̂_{i,t} − q_t to avoid realizing losses.\n",
    "    \n",
    "        Parameters:\n",
    "            tPos (float): target position\n",
    "            pos (float): current position\n",
    "            f (float): forecast price\n",
    "            w (float): coefficient that regulates the width of the sigmoid function\n",
    "            maxPos (float): maximum absolute position size\n",
    "        \n",
    "        Returns:\n",
    "            lP (float): limit price\n",
    "    '''\n",
    "    sgn = (1 if tPos >= pos else -1)\n",
    "    lP = 0\n",
    "    for j in range(abs(pos + sgn), abs(tPos + 1)):\n",
    "        lP += inv_price(f, w, j / float(maxPos))\n",
    "    lP /= tPos - pos\n",
    "    return lP\n",
    "\n",
    "\n",
    "def get_w(x: float, m: float):\n",
    "    '''\n",
    "    Calibrates sigmoid coefficient by calculating the inverse function of bet size with respect to w.\n",
    "    \n",
    "        Parameters:\n",
    "            x (float): difference between forecast price and current price f_i - p_t\n",
    "            m (float): bet size\n",
    "    '''\n",
    "    return x ** 2 * (m**(-2) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_conc_bets_by_date(date: Timestamp, signals: pd.DataFrame) -> Tuple[int, int]:\n",
    "    '''\n",
    "    Derives number of long and short concurrent bets by given date.\n",
    "    \n",
    "        Parameters:\n",
    "            date (Timestamp): date of signal\n",
    "            signals (pd.DataFrame): dataframe with signals\n",
    "            \n",
    "        Returns:\n",
    "            long, short (Tuple[int, int]): number of long and short concurrent bets\n",
    "    '''\n",
    "    long, short = 0, 0\n",
    "    for ind in pd.date_range(start=max(signals.index[0], date - timedelta(days=25)), end=date):\n",
    "        if ind <= date and signals.loc[ind]['t1'] >= date:\n",
    "            if signals.loc[ind]['signal'] >= 0:\n",
    "                long += 1\n",
    "            else:\n",
    "                short += 1\n",
    "    return long, short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13. Backtesting on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(\n",
    "    coeffs: Dict[str, float], nIter: int = 1e4, maxHP: int = 100, rPT: np.ndarray = np.linspace(0.5, 10, 20),\n",
    "    rSLm: np.ndarray = np.linspace(0.5, 10, 20), seed: float = 0.0\n",
    ") -> list:\n",
    "    '''\n",
    "    Computes a 20×20 mesh of Sharpe ratios, one for each trading rule, given a pair of initial parameters.\n",
    "    \n",
    "        Parameters:\n",
    "            coeffs (dict): dictionary with values of forecast price, half-life of the process and sigma parameter\n",
    "            nIter (int): number of paths to simulate\n",
    "            maxHP (int): maximum holding period\n",
    "            rPT (np.ndarray): profit take upper bound (in std units)\n",
    "            rSLm (np.ndarray): stop loss lower bound (in std units)\n",
    "            seed (float): initial price P_{i, t} (can be fixed to 0, drives the converges only and not absolute values)\n",
    "            \n",
    "        Returns:\n",
    "            output1 (list): array contatining bounds combination and strategy performance\n",
    "    '''\n",
    "    phi, output1 = 2 ** (-1.0 / coeffs['hl']), []\n",
    "    for comb_ in product(rPT, rSLm):\n",
    "        output2 = []\n",
    "        for iter_ in range(int(nIter)):\n",
    "            p, hp, count = seed, 0, 0\n",
    "            while True:\n",
    "                p = (1 - phi) * coeffs['forecast'] + phi * p + coeffs['sigma'] * gauss(0, 1)\n",
    "                cP = p - seed\n",
    "                hp += 1\n",
    "                if cP > comb_[0] or cP < -comb_[1] or hp > maxHP:\n",
    "                    output2.append(cP)\n",
    "                    break\n",
    "        mean, std = np.mean(output2), np.std(output2)\n",
    "        output1.append((comb_[0], comb_[1], mean, std, mean / std))\n",
    "    return output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_parameters():\n",
    "    '''\n",
    "    Iterates over different combinations of initial parameters and run simulations for each.\n",
    "    '''\n",
    "    rPT = rSLm = np.linspace(0,10,21)\n",
    "    count = 0\n",
    "    for prod_ in product([10, 5, 0, -5, -10], [5, 10, 25, 50, 100]):\n",
    "        count += 1\n",
    "        coeffs = {'forecast': prod_[0], 'hl': prod_[1], 'sigma': 1}\n",
    "        output = batch(coeffs, nIter=1e4, maxHP=100, rPT=rPT, rSLm=rSLm)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14. Backtest Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bets_timing(tPos: pd.Series) -> pd.Index:\n",
    "    '''\n",
    "    Calculates the timestamps of flattening or flipping trades from target positions series.\n",
    "    \n",
    "    Parameters:\n",
    "        tPos (pd.Series): series with target positions\n",
    "        \n",
    "    Returns:\n",
    "        bets (pd.Index): bets timing\n",
    "    '''\n",
    "    df0 = tPos[tPos == 0].index\n",
    "    df1 = tPos.shift(1)\n",
    "    df1 = df1[df1 != 0].index\n",
    "    bets = df0.intersection(df1)    # flattening\n",
    "    df0 = tPos.iloc[1:] * tPos.iloc[:-1].values\n",
    "    bets = bets.union(df0[df0 < 0].index).sort_values()    # tPos flips\n",
    "    if tPos.index[-1] not in bets:\n",
    "        bets = bets.append(tPos.index[-1:])    # last bet\n",
    "    return bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_holding_period(tPos: pd.Series) -> float:\n",
    "    '''\n",
    "    Derives average holding period (in days) using average entry time pairing algo.\n",
    "    \n",
    "    Parameters:\n",
    "        tPos (pd.Series): series with target positions\n",
    "        \n",
    "    Returns:\n",
    "        hp (float): holding period\n",
    "    '''\n",
    "    hp, tEntry = pd.DataFrame(columns=['dT', 'w']), 0.0\n",
    "    pDiff, tDiff = tPos.diff(), (tPos.index - tPos.index[0]) / np.timedelta64(1, 'D')\n",
    "    for i in range(1, tPos.shape[0]):\n",
    "        if pDiff.iloc[i] * tPos.iloc[i - 1] >= 0:    # increased or unchanged\n",
    "            if tPos.iloc[i] != 0:\n",
    "                tEntry = (tEntry * tPos.iloc[i - 1] + tDiff[i] * pDiff.iloc[i]) / tPos.iloc[i]\n",
    "        else:    # decreased\n",
    "            if tPos.iloc[i] * tPos.iloc[i-1] < 0:    # flip\n",
    "                hp.loc[tPos.index[i], ['dT', 'w']] = (tDiff[i] - tEntry, abs(tPos.iloc[i - 1]))\n",
    "                tEntry = tDiff[i]    # reset entry time\n",
    "            else:\n",
    "                hp.loc[tPos.index[i], ['dT', 'w']] = (tDiff[i] - tEntry, abs(pDiff.iloc[i]))\n",
    "    if hp['w'].sum() > 0:\n",
    "        hp = (hp['dT'] * hp['w']).sum() / hp['w'].sum()\n",
    "    else:\n",
    "        hp = np.nan\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HHI(betRet: pd.Series) -> float:\n",
    "    '''\n",
    "    Derives HHI concentration of returns (see p. 200 for definition). Returns can be divided into positive\n",
    "    and negative or you can calculate the concentration of bets across the months.\n",
    "    \n",
    "    Parameters:\n",
    "        betRet (pd.Series): series with bets returns\n",
    "        \n",
    "    Returns:\n",
    "        hhi (float): concentration\n",
    "    '''\n",
    "    if betRet.shape[0] <= 2:\n",
    "        return np.nan\n",
    "    wght = betRet / betRet.sum()\n",
    "    hhi = (wght ** 2).sum()\n",
    "    hhi = (hhi - betRet.shape[0] ** (-1)) / (1.0 - betRet.shape[0] ** (-1))\n",
    "    return hhi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_DD_TuW(series: pd.Series, dollars: bool = False) -> Tuple[pd.Series, pd.Series]:\n",
    "    '''\n",
    "     Computes series of drawdowns and the time under water associated with them.\n",
    "    \n",
    "    Parameters:\n",
    "        series (pd.Series): series with either returns (dollars=False) or dollar performance (dollar=True)\n",
    "        dollars (bool): indicator charachterizing series\n",
    "        \n",
    "    Returns:\n",
    "        dd (pd.Series): drawdown series\n",
    "        tuw (pd.Series): time under water series\n",
    "    '''\n",
    "    df0 = series.to_frame('pnl')\n",
    "    df0['hwm'] = series.expanding().max()\n",
    "    df1 = df0.groupby('hwm').min().reset_index()\n",
    "    df1.columns = ['hwm', 'min']\n",
    "    df1.index = df0['hwm'].drop_duplicates(keep='first').index    # time of hwm\n",
    "    df1 = df1[df1['hwm'] > df1['min']]    # hwm followed by a drawdown\n",
    "    if dollars:\n",
    "        dd = df1['hwm'] - df1['min']\n",
    "    else:\n",
    "        dd = 1 - df1['min'] / df1['hwm']\n",
    "    tuw = ((df1.index[1:] - df1.index[:-1]) / np.timedelta64(1, 'Y')).values    # in years\n",
    "    tuw = pd.Series(tuw, index=df1.index[:-1])\n",
    "    return dd, tuw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15. Understanding Strategy Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_SR(prob: float, sl: float, pt: float, freq: float, num_trials: int = 1000000) -> float:\n",
    "    '''\n",
    "    Estimates strategy's Sharpe ratio under given parameters.\n",
    "    \n",
    "        Parameters:\n",
    "            prob (float): precision of the strategy\n",
    "            sl (float): stop loss threshold\n",
    "            pt (float): profit taking threshold\n",
    "            freq (float): annual number of bets (to obtain annualized SR)\n",
    "            num_trial (int): number of trials used for estimation\n",
    "            \n",
    "        Returns:\n",
    "            sr (float): Sharpe ratio\n",
    "    '''\n",
    "    out = []\n",
    "    for i in range(num_trials):\n",
    "        rnd = np.random.binomial(n=1, p=prob)\n",
    "        if rnd == 1:\n",
    "            x = pt\n",
    "        else:\n",
    "            x = sl\n",
    "        out.append(x)\n",
    "    sr = np.mean(out) / np.std(out) * np.sqrt(freq)\n",
    "    return sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_HR(sl: float, pt: float, freq: float, tSR: float) -> float:\n",
    "    '''\n",
    "    Returns minimum precision p needed to achieve target Sharpe ration under given parameters.\n",
    "    \n",
    "        Parameters:\n",
    "            sl (float): stop loss threshold\n",
    "            pt (float): profit taking threshold\n",
    "            freq (float): annual number of bets\n",
    "            tSR (float): target annual Sharpe ratio\n",
    "            \n",
    "        Returns:\n",
    "            p (float): precision\n",
    "    '''\n",
    "    a = (freq + tSR ** 2) * (pt - sl) ** 2\n",
    "    b = (2 * freq * sl - tSR ** 2 * (pt - sl)) * (pt - sl)\n",
    "    c = freq * sl ** 2\n",
    "    p = (-b + (b ** 2 - 4 * a * c) ** 0.5) / (2.0 * a)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_freq(sl: float, pt: float, p: float, tSR: float) -> float:\n",
    "    '''\n",
    "    Returns minimum number of bets per year needed to achieve target Sharpe ration under given parameters.\n",
    "    \n",
    "        Parameters:\n",
    "            sl (float): stop loss threshold\n",
    "            pt (float): profit taking threshold\n",
    "            p (float): precision\n",
    "            tSR (float): target annual Sharpe ratio\n",
    "            \n",
    "        Returns:\n",
    "            freq (float): annual number of bets\n",
    "    '''\n",
    "    freq = (tSR * (pt - sl)) ** 2 * p * (1 - p) / ((pt - sl) * p + sl) ** 2\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_gaussians(\n",
    "    mu1: float, mu2: float, sigma1: float, sigma2: float, prob1: float, nObs: int\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Generates random draws form a mixture of two Gaussians.\n",
    "    \n",
    "        Parameters:\n",
    "            mu1 (float): expectation of 1st Gaussian\n",
    "            mu2 (float): expectation of 2nd Gaussian\n",
    "            sigma1 (float): std of 1st Gaussian\n",
    "            sigma2 (float): std of 2nd Gaussian\n",
    "            prob1 (float): probability of generating from 1st Gaussian (i.e. weight of 1st Gaussian)\n",
    "            nObs (int): total number of draws\n",
    "            \n",
    "        Returns:\n",
    "            ret (np.ndarray): array with observations\n",
    "    '''\n",
    "    ret1 = np.random.normal(mu1, sigma1, size=int(nObs * prob1))\n",
    "    ret2 = np.random.normal(mu2, sigma2, size=nObs - ret1.shape[0])\n",
    "    ret = np.append(ret1, ret2, axis=0)\n",
    "    np.random.shuffle(ret)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_failure(ret: np.ndarray, freq: float, tSR: float):\n",
    "    '''\n",
    "    Derives probability that strategy has lower precision than needed.\n",
    "    \n",
    "        Parameters:\n",
    "            ret (np.ndarray): array with observations\n",
    "            freq (float): annual number of bets\n",
    "            tSR (float): target Sharpe ratio\n",
    "            \n",
    "        Returns:\n",
    "            risk (float): probability of failure\n",
    "    '''\n",
    "    rPos, rNeg = ret[ret > 0].mean(), ret[ret <= 0].mean()\n",
    "    p = ret[ret > 0].shape[0] / float(ret.shape[0])\n",
    "    thresP = bin_HR(rNeg, rPos, freq, tSR)\n",
    "    risk = norm.cdf(thresP, p, p * (1 - p))    # approximation to bootstrap\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16. Machine Learning Asset Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ivp(cov: np.ndarray, **kargs) -> np.ndarray:\n",
    "    '''\n",
    "    Computes the inverse variance portfolio.\n",
    "    \n",
    "        Parameters:\n",
    "            cov (np.ndarray): covariance matrix\n",
    "            \n",
    "        Returns:\n",
    "            ivp (np.ndarray): optimal portfolio weights\n",
    "    '''\n",
    "    ivp = 1.0 / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_var(cov: np.ndarray, cItems: np.ndarray) -> float:\n",
    "    '''\n",
    "    Computes variance per cluster\n",
    "    \n",
    "        Parameters:\n",
    "            cov (np.ndarray): covariance matrix for all items\n",
    "            cItems (np.ndarray): indexes of cluster items\n",
    "            \n",
    "        Returns:\n",
    "            cVar (float): cluster variance\n",
    "    '''\n",
    "    cov_ = cov.loc[cItems, cItems]    # matrix slice\n",
    "    w_ = get_ivp(cov_).reshape(-1, 1)\n",
    "    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
    "    return cVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quasi_diag(link: np.ndarray) -> list:\n",
    "    '''\n",
    "    Performs Quasi-Diagonalization by sorting clustered items by distance.\n",
    "    \n",
    "        Parameters:\n",
    "            link (np.ndarray): a linkage matrix of size (N−1)x4\n",
    "        \n",
    "        Returns:\n",
    "            lst (list): sorted items list\n",
    "    '''\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    numItems = link[-1, 3]    # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)    # make space\n",
    "        df0 = sortIx[sortIx >= numItems]    # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j, 0]    # item 1\n",
    "        df0 = pd.Series(link[j, 1], index=i+1)\n",
    "        sortIx = sortIx.append(df0)    # item 2\n",
    "        sortIx = sortIx.sort_index()    # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0])    # re-index\n",
    "    lst =  sortIx.tolist()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_bipart(cov: np.ndarray, sortIx: list) -> pd.Series:\n",
    "    '''\n",
    "    Computes Hierarchical Risk Parity allocation for a given subset of items.\n",
    "    \n",
    "        Parameters:\n",
    "            cov (np.ndarray): covariance matrix\n",
    "            sortIx (list): sorted items list\n",
    "    '''\n",
    "    w = pd.Series([1] * len(sortIx), index=sortIx)\n",
    "    cItems = [sortIx]    # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        cItems = [i[int(j): int(k)] for i in cItems\n",
    "                  for j, k in ((0, len(i) / 2), (len(i) / 2, len(i))) if len(i) > 1]    # bi-section\n",
    "        for i in range(0, len(cItems), 2):    # parse in pairs\n",
    "            cItems0 = cItems[i]    # cluster 1\n",
    "            cItems1 = cItems[i+1]    # cluster 2\n",
    "            cVar0 = get_cluster_var(cov, cItems0)\n",
    "            cVar1 = get_cluster_var(cov, cItems1)\n",
    "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "            w[cItems0] *= alpha    # weight 1\n",
    "            w[cItems1] *= 1 - alpha    # weight 2\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correl_dist(corr: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Calculates a distance matrix based on correlation, where 0<=d[i,j]<=1. This is a proper distance metric.\n",
    "    \n",
    "        Parameters:\n",
    "            corr (np.ndarray): correlation matrix\n",
    "        \n",
    "        Returns:\n",
    "            dist (np.ndarray): distance matrix\n",
    "    '''\n",
    "    dist = ((1 - corr) / 2.0) ** 0.5    # distance matrix\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_matrix(corr: np.ndarray, labels: list = None, size: tuple = (9, 9)) -> None:\n",
    "    '''\n",
    "    Plots heatmap of the correlation matrix.\n",
    "    \n",
    "        Parameters:\n",
    "            corr (np.ndarray): correlation matrix\n",
    "            labels (list): labels for items\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=size)\n",
    "    if labels is None:\n",
    "        labels = []\n",
    "    ax = sns.heatmap(corr)\n",
    "    ax.set_yticks(np.arange(0.5, corr.shape[0] + 0.5), list(labels))\n",
    "    ax.set_xticks(np.arange(0.5, corr.shape[0] + 0.5), list(labels))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(nObs: int, size0: int, size1: int, sigma1: float) -> Tuple[pd.DataFrame, list]:\n",
    "    '''\n",
    "    Generates data with correlations.\n",
    "    \n",
    "        Parameters:\n",
    "            nObs (int): number of observations\n",
    "            size0 (int): number of uncorrelated items\n",
    "            size1 (int): number of correlated items\n",
    "            sigma1 (float): std for random noise\n",
    "            \n",
    "        Returns:\n",
    "            x (pd.DataFrame): dataframe with generated data\n",
    "            cols (list): list with index of correlated items for each of the item in the list\n",
    "    '''\n",
    "    #1) generating some uncorrelated data\n",
    "    np.random.seed(seed=42)\n",
    "    random.seed(42)\n",
    "    x = np.random.normal(0, 1, size=(nObs, size0))    # each row is a variable\n",
    "    #2) creating correlation between the variables\n",
    "    cols = [random.randint(0, size0 - 1) for i in range(size1)]\n",
    "    y = x[:, cols] + np.random.normal(0, sigma1, size=(nObs, len(cols)))\n",
    "    x = np.append(x, y, axis=1)\n",
    "    x = pd.DataFrame(x, columns=range(1, x.shape[1] + 1))\n",
    "    return x, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation() -> None:\n",
    "    '''\n",
    "    Runs simulation and performs HRP algorithm.\n",
    "    '''\n",
    "    #1) Generate correlated data\n",
    "    nObs, size0, size1, sigma1 = 10000, 5, 5, 0.25\n",
    "    x, cols = generate_data(nObs, size0, size1, sigma1)\n",
    "    print([(j + 1, size0 + i) for i, j in enumerate(cols, 1)])\n",
    "    cov, corr = x.cov(), x.corr()\n",
    "    #2) compute and plot correl matrix\n",
    "    plot_corr_matrix(corr, labels=corr.columns, size=(8, 6.5))\n",
    "    #3) cluster\n",
    "    dist = correl_dist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = get_quasi_diag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()    # recover labels\n",
    "    df0 = corr.loc[sortIx, sortIx]    # reorder\n",
    "    plot_corr_matrix(df0, labels=df0.columns, size=(8, 6.5))\n",
    "    #4) Capital allocation\n",
    "    hrp = get_rec_bipart(cov, sortIx)\n",
    "    print(hrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_mc(\n",
    "    nObs: int, sLength: int, size0: int, size1: int, mu0: float, sigma0: float, sigma1F: float\n",
    ") -> Tuple[np.ndarray, list]:\n",
    "    '''\n",
    "    Generates data with two types of random shocks:\n",
    "    common to various investments and specific to a single investment.\n",
    "    \n",
    "    Parameters:\n",
    "            nObs (int): number of observations\n",
    "            sLength (int): period length to compute HRP and IVP\n",
    "            size0 (int): number of uncorrelated items\n",
    "            size1 (int): number of correlated items\n",
    "            mu0 (float): mean for generated uncorrelated data\n",
    "            sigma0 (float): std for random noise\n",
    "            sigma1F (float): multiplier for correlation noise\n",
    "            \n",
    "        Returns:\n",
    "            x (np.ndarray): matrix with generated data\n",
    "            cols (list): list with index of correlated items for each of the item in the list\n",
    "    '''\n",
    "    #1) generate random uncorrelated data\n",
    "    x = np.random.normal(mu0, sigma0, size=(nObs, size0))\n",
    "    #2) create correlation between the variables\n",
    "    cols = [random.randint(0, size0 - 1) for i in range(size1)]\n",
    "    y = x[:, cols] + np.random.normal(0, sigma0 * sigma1F, size=(nObs, len(cols)))\n",
    "    x = np.append(x, y, axis=1)\n",
    "    #3) add common random shock\n",
    "    point = np.random.randint(sLength, nObs - 1, size=2)\n",
    "    x[np.ix_(point, [cols[0], size0])] = np.array([[-0.5, -0.5], [2, 2]])\n",
    "    #4) add specific random shock\n",
    "    point = np.random.randint(sLength, nObs - 1, size=2)\n",
    "    x[point, cols[-1]] = np.array([-0.5, 2])\n",
    "    return x, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrp(cov: np.ndarray, corr: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Constructs a hierarchical portfolio.\n",
    "    \n",
    "        Parameters:\n",
    "            cov (np.ndarray): covariance matrix\n",
    "            corr (np.ndarray): correlation matrix\n",
    "            \n",
    "        Returns:\n",
    "            hrp (pd.Series): portfolio weight given by HRP method\n",
    "    '''\n",
    "    corr, cov = pd.DataFrame(corr), pd.DataFrame(cov)\n",
    "    dist = correl_dist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = get_quasi_diag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()    # recover labels\n",
    "    hrp = get_rec_bipart(cov,sortIx)\n",
    "    return hrp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrp_mc(\n",
    "    numIters: int = 1e2, nObs: int = 520, size0: int = 5, size1: int = 5, mu0: float = 0,\n",
    "    sigma0: float = 1e-2, sigma1F: float = 0.25, sLength: int = 260, rebal: int = 22\n",
    ") -> None:\n",
    "    '''\n",
    "    Performs Monte Carlo experiment on HRP method.\n",
    "    \n",
    "    Parameters:\n",
    "        numIters (int): number of Monte Carlo iterations\n",
    "        nObs (int): number of observations\n",
    "        size0 (int): number of uncorrelated items\n",
    "        size1 (int): number of correlated items\n",
    "        mu0 (float): mean for generated uncorrelated data\n",
    "        sigma0 (float): std for random noise\n",
    "        sigma1F (float): multiplier for correlation noise\n",
    "        sLength (int): period length to compute HRP and IVP\n",
    "        rebal (int): rebalancing frequency (after how many periods we rebalance portfolios)\n",
    "    '''\n",
    "    methods = [get_ivp, get_hrp]\n",
    "    stats, numIter = {i.__name__: pd.Series() for i in methods}, 0\n",
    "    pointers = range(sLength, nObs, rebal)\n",
    "    while numIter < numIters:\n",
    "        #1) Prepare data for one experiment\n",
    "        x, cols = generate_data_mc(nObs, sLength, size0, size1, mu0, sigma0, sigma1F)\n",
    "        r = {i.__name__: pd.Series() for i in methods}\n",
    "        #2) Compute portfolios in-sample\n",
    "        for pointer in pointers:\n",
    "            x_ = x[pointer - sLength: pointer]\n",
    "            cov_, corr_ = np.cov(x_, rowvar=0), np.corrcoef(x_, rowvar=0)\n",
    "            #3) Compute performance out-of-sample\n",
    "            x_ = x[pointer: pointer + rebal]\n",
    "            for func in methods:\n",
    "                w_ = func(cov=cov_, corr=corr_)    # callback\n",
    "                r_ = pd.Series(np.dot(x_, w_))\n",
    "                r[func.__name__] = r[func.__name__].append(r_)\n",
    "        #4) Evaluate and store results\n",
    "        for func in methods:\n",
    "            r_ = r[func.__name__].reset_index(drop=True)\n",
    "            p_ = (1 + r_).cumprod()\n",
    "            stats[func.__name__].loc[numIter] = p_.iloc[-1] - 1\n",
    "        numIter += 1\n",
    "    #5) Report results\n",
    "    stats = pd.DataFrame.from_dict(stats, orient='columns')\n",
    "    df0, df1 = stats.std(), stats.var()\n",
    "    print(pd.concat([df0, df1, df1 / df1['get_hrp'] - 1], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17. Structural Breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_betas(y: np.ndarray, x: np.ndarray) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Carries out the actual regressions.\n",
    "    \n",
    "        Parameters:\n",
    "            y (np.ndarray): y array\n",
    "            x (np.ndarray): x matrix\n",
    "            \n",
    "        Returns:\n",
    "            bMean (float): estimate of the mean of the beta coefficient\n",
    "            bVar (float): estimate of the variance of the beta coefficient\n",
    "    '''\n",
    "    xy = np.dot(x.T, y)\n",
    "    xx = np.dot(x.T, x)\n",
    "    xxinv = np.linalg.inv(xx)\n",
    "    bMean = np.dot(xxinv, xy)\n",
    "    err = y - np.dot(x, bMean)\n",
    "    bVar = np.dot(err.T, err) / (x.shape[0] - x.shape[1]) * xxinv\n",
    "    return bMean, bVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_DF(df0: pd.DataFrame, lags: Union[int, list]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Applies specified lags to a dataframe.\n",
    "    \n",
    "        Parameters:\n",
    "            df0 (pd.DataFrame): dataframe to which lags are applied\n",
    "            lags (Union[int, list]): lags\n",
    "            \n",
    "        Returns:\n",
    "            df1 (pd.DataFrame): transformed dataframe\n",
    "    '''\n",
    "    df1 = pd.DataFrame()\n",
    "    if isinstance(lags, int):\n",
    "        lags = range(lags + 1)\n",
    "    else:\n",
    "        lags = [int(lag) for lag in lags]\n",
    "    for lag in lags:\n",
    "        df_ = df0.shift(lag).copy(deep=True)\n",
    "        df_.columns = [str(i) + '_' + str(lag) for i in df_.columns]\n",
    "        df1 = df1.join(df_, how='outer')\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_YX(series: pd.Series, constant: str, lags: Union[int, list]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Prepares numpy objects needed to conduct the recursive texts.\n",
    "    \n",
    "        Parameters:\n",
    "            series (pd.Series): series (containing prices)\n",
    "            constant (str): the regression's time trend component\n",
    "                                - 'nc': no time trend, only a constant\n",
    "                                - 'ct': a constant plus a linear time trend\n",
    "                                - 'ctt': a constant plus a second-degree polynomial time trend\n",
    "            lags (Union[int, list]): the number of lags used in the ADF specification\n",
    "        \n",
    "        Returns:\n",
    "            y (np.ndarray): y array\n",
    "            x (np.ndarray): x matrix\n",
    "    '''\n",
    "    series_ = series.diff().dropna()\n",
    "    x = lag_DF(series_, lags).dropna()\n",
    "    x.iloc[:, 0] = series.values[-x.shape[0] - 1: -1, 0]    # lagged level\n",
    "    y = series_.iloc[-x.shape[0]:].values\n",
    "    if constant != 'nc':\n",
    "        x = np.append(x, np.ones((x.shape[0], 1)), axis=1)\n",
    "        if constant[:2] == 'ct':\n",
    "            trend = np.arange(x.shape[0]).reshape(-1, 1)\n",
    "            x = np.append(x, trend, axis=1)\n",
    "        if constant == 'ctt':\n",
    "            x = np.append(x, trend ** 2, axis=1)\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bsdaf(logP: pd.Series, minSL: int, constant: str, lags: Union[int, list]) -> dict:\n",
    "    '''\n",
    "    Conducts SDAF inner loop.\n",
    "    \n",
    "        Parameters:\n",
    "            logP (pd.Series): series containing log prices\n",
    "            minSL (int): minimum sample length used by the final regression\n",
    "            constant (str): the regression's time trend component\n",
    "                                - 'nc': no time trend, only a constant\n",
    "                                - 'ct': a constant plus a linear time trend\n",
    "                                - 'ctt': a constant plus a second-degree polynomial time trend\n",
    "            lags (Union[int, list]): the number of lags used in the ADF specification\n",
    "        \n",
    "        Returns:\n",
    "            out (dict): dictionary with time and SADF_t estimation\n",
    "    '''\n",
    "    y, x = get_YX(logP, constant=constant, lags=lags)\n",
    "    startPoints, bsadf, allADF = range(0, y.shape[0] + lags - minSL + 1), None, []\n",
    "    for start in startPoints:\n",
    "        y_, x_ = y[start:], x[start:]\n",
    "        bMean_, bStd_ = get_betas(y_, x_)\n",
    "        bMean_, bStd_ = bMean_[0, 0], bStd_[0, 0] ** 0.5\n",
    "        allADF.append(bMean_ / bStd_)\n",
    "        if allADF[-1] > bsadf:\n",
    "            bsadf = allADF[-1]\n",
    "    out = {'Time': logP.index[-1], 'gsadf': bsadf}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18. Entropy Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmf1(msg: Any, w: int) -> dict:\n",
    "    '''\n",
    "    Computes the probability mass function for a one-dim random variable (len(msg) - w occurences).\n",
    "    \n",
    "        Parameters:\n",
    "            msg (Any): sequence with observations (usually a string)\n",
    "            w (int): word length used for pmf estimation\n",
    "            \n",
    "        Returns:\n",
    "            pmf (dict): dictionary with words as keys and their estimated probabilities as values.\n",
    "    '''\n",
    "    lib = {}\n",
    "    if not isinstance(msg, str):\n",
    "        msg = ''.join(map(str, msg))\n",
    "    for i in range(w, len(msg)):\n",
    "        msg_ = msg[i - w: i]\n",
    "        if msg_ not in lib:\n",
    "            lib[msg_] = [i - w]\n",
    "        else:\n",
    "            lib[msg_] = lib[msg_] + [i - w]\n",
    "    length = float(len(msg) - w)\n",
    "    pmf = {i: len(lib[i]) / length for i in lib}\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plug_in(msg: Any, w: int) -> Tuple[float, dict]:\n",
    "    '''\n",
    "    Computes the maximum likelihood estimate for the entropy rate.\n",
    "\n",
    "        Parameters:\n",
    "            msg (Any): sequence with observations (usually a string)\n",
    "            w (int): word length used for pmf estimation\n",
    "\n",
    "        Returns:\n",
    "            out (float): entropy estimate\n",
    "            pmf (dict): dictionary with words as keys and their estimated probabilities as values.\n",
    "    '''\n",
    "    pmf = pmf1(msg, w)\n",
    "    out = -sum([pmf[i] * np.log2(pmf[i]) for i in pmf]) / w\n",
    "    return out, pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lempel_ziv_lib(msg: str) -> list:\n",
    "    '''\n",
    "    Implements the LZ algorithm to construct library.\n",
    "    \n",
    "        Parameters:\n",
    "            msg (str): sequence with observations\n",
    "            \n",
    "        Returns:\n",
    "            lib (list): list containing unique words\n",
    "    '''\n",
    "    i, lib = 1, [msg[0]]\n",
    "    while i < len(msg):\n",
    "        for j in range(i, len(msg)):\n",
    "            msg_ = msg[i: j + 1]\n",
    "            if msg_ not in lib:\n",
    "                lib.append(msg_)\n",
    "                break\n",
    "        i = j + 1\n",
    "    return lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_length(msg: str, i: int, n: int) -> Tuple[int, str]:\n",
    "    '''\n",
    "    Computes the length of the longest match.\n",
    "    \n",
    "        Parameters:\n",
    "            msg (str): sequence with observations\n",
    "            i (int): position before which we look for a match\n",
    "            n (int): size of the window for searching for a match\n",
    "            \n",
    "        Returns:\n",
    "            len(subS) + 1 (int): length of the match + 1\n",
    "            subS (str): matched substring\n",
    "    '''\n",
    "    subS = ''\n",
    "    for l in range(n):\n",
    "        msg1 = msg[i: i + 1 + l]\n",
    "        for j in range(i - n, i):\n",
    "            msg0 = msg[j: j + 1 + l]\n",
    "            if msg1 == msg0:\n",
    "                subS = msg1\n",
    "                break\n",
    "    return len(subS) + 1, subS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def konto(msg: Any, window: Optional[int] = None) -> dict:\n",
    "    '''\n",
    "    Kontoyiannis' LZ entropy estimate, 2013 version (centered window). Inverse of the avg length of the shortest\n",
    "    non redundant substring. If non redundant substrings are short, the text is highly entropic.\n",
    "    window=None for expanding window , in which case len(msg)%2=0.\n",
    "    If the end of the message is more relevant, try conto(msg[::-1]).\n",
    "    \n",
    "        Parameters:\n",
    "            msg (Any): sequence with observations (usually a string)\n",
    "            window (Optional[int]): winodw size for constant window\n",
    "            \n",
    "        Returns:\n",
    "            out (dict): dictionary with results\n",
    "    '''\n",
    "    out = {'num': 0, 'sum': 0, 'subS': []}\n",
    "    if not isinstance(msg, str):\n",
    "        msg = ''.join(map(str, msg))\n",
    "    if window is None:\n",
    "        points = range(1, len(msg) // 2 + 1)\n",
    "    else:\n",
    "        window = min(window, len(msg) // 2)\n",
    "        points = range(window, len(msg) - window + 1)\n",
    "    for i in points:\n",
    "        if window is None:\n",
    "            l, msg_ = match_length(msg, i, i)\n",
    "            out['sum'] += np.log2(i + 1) / l    # to avoid Doeblin condition\n",
    "        else:\n",
    "            l, msg_ = match_length(msg, i, window)\n",
    "            out['sum'] += np.log2(window + 1) / l    # to avoid Doeblin condition\n",
    "        out['subS'].append(msg_)\n",
    "        out['num'] += 1\n",
    "    out['h'] = out['sum'] / out['num']\n",
    "    out['r'] = 1 - out['h'] / np.log2(len(msg))    # redundancy, 0 <= r <= 1\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
