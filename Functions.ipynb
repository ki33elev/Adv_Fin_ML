{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains all the functions implemented through the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Tuple, List, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Financial Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaWeights(cov: np.ndarray, riskDist: np.ndarray = None,\n",
    "               riskTarget: float = 1.) -> np.ndarray:\n",
    "    eVal, eVec = np.linalg.eigh(cov)\n",
    "    indices = eVal.argsort()[::-1]\n",
    "    eVal, eVec = eVal[indices], eVec[:, indices]    # sorting by decreasing eVal (i.e. decreasing variance)\n",
    "    if riskDist is None:\n",
    "        riskDist = np.zeros(cov.shape[0])\n",
    "        riskdist[-1] = 1.\n",
    "    loads = riskTarget * (riskDist / eVal) ** 0.5\n",
    "    weights = np.dot(eVec, np.reshape(loads, (-1, 1)))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetrical CUSUM filter\n",
    "def getTEvents(gRaw: pd.Series, h: float) -> np.ndarray:\n",
    "    gRaw = gRaw[~gRaw.index.duplicated(keep='first')]\n",
    "    tEvents, sPos, sNeg = [], 0, 0\n",
    "    diff = gRaw.diff()\n",
    "    for i in diff.index[1:]:\n",
    "        sPos, sNeg = max(0, sPos + diff.loc[i]), min(0, sNeg + diff.loc[i])\n",
    "        if sNeg < -h:\n",
    "            sNeg = 0\n",
    "            tEvents.append(i)\n",
    "        elif sPos > h:\n",
    "            sPos = 0\n",
    "            tEvents.append(i)\n",
    "    return pd.DatetimeIndex(tEvents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://towardsdatascience.com/advanced-candlesticks-for-machine-learning-i-tick-bars-a8b93728b4c5\n",
    "def get_tick_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                  times: np.ndarray, freq: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(range(freq, len(prices), freq)), 6), dtype=object)\n",
    "    ind = 0\n",
    "    for i in range(freq, len(prices), freq):\n",
    "        bars[ind][0] = pd.Timestamp(times[i - 1])          # time\n",
    "        bars[ind][1] = prices[i - freq]                    # open\n",
    "        bars[ind][2] = np.max(prices[i - freq: i])         # high\n",
    "        bars[ind][3] = np.min(prices[i - freq: i])         # low\n",
    "        bars[ind][4] = prices[i - 1]                       # close\n",
    "        bars[ind][5] = np.sum(vols[i - freq: i])           # volume\n",
    "        ind += 1\n",
    "    return bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                    times: np.ndarray, bar_vol: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(prices), 6), dtype=object)\n",
    "    ind = 0\n",
    "    last_tick = 0\n",
    "    cur_volume = 0\n",
    "    for i in range(len(prices)):\n",
    "        cur_volume += vols[i]\n",
    "        if cur_volume >= bar_vol:\n",
    "            bars[ind][0] = pd.Timestamp(times[i - 1])            # time\n",
    "            bars[ind][1] = prices[last_tick]                     # open\n",
    "            bars[ind][2] = np.max(prices[last_tick: i + 1])      # high\n",
    "            bars[ind][3] = np.min(prices[last_tick: i + 1])      # low\n",
    "            bars[ind][4] = prices[i]                             # close\n",
    "            bars[ind][5] = np.sum(vols[last_tick: i + 1])        # volume\n",
    "            cur_volume = 0\n",
    "            last_tick = i + 1\n",
    "            ind += 1\n",
    "    return bars[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dollar_bars(prices: np.ndarray, vols: np.ndarray,\n",
    "                    times: np.ndarray, bar_sum: int) -> np.ndarray:\n",
    "    bars = np.zeros(shape=(len(prices), 6), dtype=object)\n",
    "    ind = 0\n",
    "    last_tick = 0\n",
    "    cur_sum = 0\n",
    "    for i in range(len(prices)):\n",
    "        cur_sum += vols[i] * prices[i]\n",
    "        if cur_sum >= bar_sum:\n",
    "            bars[ind][0] = pd.Timestamp(times[i - 1])            # time\n",
    "            bars[ind][1] = prices[last_tick]                     # open\n",
    "            bars[ind][2] = np.max(prices[last_tick: i + 1])      # high\n",
    "            bars[ind][3] = np.min(prices[last_tick: i + 1])      # low\n",
    "            bars[ind][4] = prices[i]                             # close\n",
    "            bars[ind][5] = np.sum(vols[last_tick: i + 1])        # volume\n",
    "            cur_sum = 0\n",
    "            last_tick = i + 1\n",
    "            ind += 1\n",
    "    return bars[:ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bollinger_bands(dollar_bars: np.ndarray, alpha: float) -> np.ndarray:\n",
    "    prices = dollar_bars[:, 4]    # taking close prices\n",
    "    ma = (pd.Series(prices).rolling(20, min_periods=20).mean())      # 20 bars moving average\n",
    "    sigma = pd.Series(prices).rolling(20, min_periods=20).std()\n",
    "    b_upper, b_lower = (ma + alpha * sigma), (ma - alpha * sigma)    # bollinger bounds    \n",
    "    return np.array([ma, b_upper, b_lower])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_returns(bars: np.ndarray) -> np.ndarray:\n",
    "    close_prices = pd.Series(bars[:, 4], index=bars[:, 0])\n",
    "    return (close_prices.diff() / close_prices)[1:, ].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_vol(close: pd.Series, span0: int = 20) -> pd.Series:\n",
    "    df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))\n",
    "    df0 = df0[df0 > 0]\n",
    "    df0 = pd.Series(close.index[df0 - 1], index=close.index[close.shape[0] - df0.shape[0]:])\n",
    "    df0 = close.loc[df0.index] / close.loc[df0.values].values - 1    # daily returns\n",
    "    df0 = df0.ewm(span=span0).std()\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tripple_barrier(close: pd.Series, events: pd.DataFrame,\n",
    "                                   pt_sl: List, molecule: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    Labeling observations using tripple-barrier method\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            events (pd.DataFrame): dataframe with columns:\n",
    "                                   - t1: The timestamp of vertical barrier (if np.nan, there will not be\n",
    "                                         a vertical barrier)\n",
    "                                   - trgt: The unit width of the horizontal barriers\n",
    "            pt_sl (list): list of two non-negative float values:\n",
    "                          - pt_sl[0]: The factor that multiplies trgt to set the width of the upper barrier.\n",
    "                                      If 0, there will not be an upper barrier.\n",
    "                          - pt_sl[1]: The factor that multiplies trgt to set the width of the lower barrier.\n",
    "                                      If 0, there will not be a lower barrier.\n",
    "            molecule (np.ndarray):  subset of event indices that will be processed by a\n",
    "                                    single thread (will be used later)\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe with columns [pt, sl, t1] corresponding to timestamps at which\n",
    "                                each barrier was touched (if it happened)\n",
    "    '''\n",
    "    events_ = events.loc[molecule]\n",
    "    out = events_[['t1']].copy(deep=True)\n",
    "    if pt_sl[0] > 0:\n",
    "        pt = pt_sl[0] * events_['trgt']\n",
    "    else:\n",
    "        pt = pd.Series(data=[np.nan] * len(events.index), index=events.index)    # NaNs\n",
    "    if pt_sl[1] > 0:\n",
    "        sl = -pt_sl[1] * events_['trgt']\n",
    "    else:\n",
    "        sl = pd.Series(data=[np.nan] * len(events.index), index=events.index)    # NaNs\n",
    "    \n",
    "    for loc, t1 in events_['t1'].fillna(close.index[-1]).iteritems():\n",
    "        df0 = close[loc: t1]                                       # path prices\n",
    "        df0 = (df0 / close[loc] - 1) * events_.at[loc, 'side']     # path returns\n",
    "        out.loc[loc, 'sl'] = df0[df0 < sl[loc]].index.min()        # earlisest stop loss\n",
    "        out.loc[loc, 'pt'] = df0[df0 > pt[loc]].index.min()        # earlisest profit taking\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including metalabeleing possibility\n",
    "def get_events_tripple_barrier(\n",
    "    close: pd.Series, tEvents: np.ndarray, pt_sl: float, trgt: pd.Series, minRet: float,\n",
    "    numThreads: int = 1, t1: Union[pd.Series, bool] = False, side: pd.Series = None\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Getting times of the first barrier touch\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            tEvents (np.ndarray): np.ndarray of timestamps that seed every barrier (they can be generated\n",
    "                                  by CUSUM filter for example)\n",
    "            pt_sl (float): non-negative float that sets the width of the two barriers (if 0 then no barrier)\n",
    "            trgt (pd.Series): s series of targets expressed in terms of absolute returns\n",
    "            minRet (float): minimum target return required for running a triple barrier search\n",
    "            numThreads (int): number of threads to use concurrently\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers (pass False\n",
    "                            to disable vertical barriers)\n",
    "            side (pd.Series) (optional): metalabels containing sides of bets\n",
    "        \n",
    "        Returns:\n",
    "            events (pd.DataFrame): dataframe with columns:\n",
    "                                       - t1: timestamp of the first barrier touch\n",
    "                                       - trgt: target that was used to generate the horizontal barriers\n",
    "                                       - side (optional): side of bets\n",
    "    '''\n",
    "    trgt = trgt.loc[trgt.index.intersection(tEvents)]\n",
    "    trgt = trgt[trgt > minRet]\n",
    "    if t1 is False:\n",
    "        t1 = pd.Series(pd.NaT, index=tEvents)\n",
    "    if side is None:\n",
    "        side_, pt_sl_ = pd.Series(np.array([1.] * len(trgt.index)), index=trgt.index), [pt_sl[0], pt_sl[0]]\n",
    "    else:\n",
    "        side_, pt_sl_ = side.loc[trgt.index.intersection(side.index)], pt_sl[:2]\n",
    "    events = pd.concat({'t1': t1, 'trgt': trgt, 'side': side_}, axis=1).dropna(subset=['trgt'])\n",
    "    df0 = apply_tripple_barrier(close, events, pt_sl_, events.index)\n",
    "#     df0 = mpPandasObj(func=apply_tripple_barrier, pdObj=('molecule', events.index),\n",
    "#                       numThreads=numThreads, close=close, events=events, pt_sl=[pt_sl, pt_sl])\n",
    "    events['t1'] = df0.dropna(how='all').min(axis=1)\n",
    "    if side is None:\n",
    "        events = events.drop('side', axis=1)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vertical_barrier(close: pd.Series, tEvents: np.ndarray, numDays: int) -> pd.Series:\n",
    "    t1 = close.index.searchsorted(tEvents + pd.Timedelta(days=numDays))\n",
    "    t1 = t1[t1 < close.shape[0]]\n",
    "    t1 = pd.Series(close.index[t1], index=tEvents[:t1.shape[0]])    # adding NaNs to the end\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# including metalabeling possibility & modified to generate 0 labels\n",
    "def get_bins(close: pd.Series, events: pd.DataFrame, t1: Union[pd.Series, bool] = False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Generating labels with possibility of knowing the side (metalabeling)\n",
    "    \n",
    "        Parameters:\n",
    "            close (pd.Series): close prices of bars\n",
    "            events (pd.DataFrame): dataframe returned by 'get_events' with columns:\n",
    "                                   - index: event starttime\n",
    "                                   - t1: event endtime\n",
    "                                   - trgt: event target\n",
    "                                   - side (optional): position side\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers (pass False\n",
    "                            to disable vertical barriers)\n",
    "        \n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe with columns:\n",
    "                                       - ret: return realized at the time of the first touched barrier\n",
    "                                       - bin: if metalabeling ('side' in events), then {0, 1} (take the bet or pass)\n",
    "                                              if no metalabeling, then {-1, 1} (buy or sell)\n",
    "    '''\n",
    "    events_ = events.dropna(subset=['t1'])\n",
    "    px = events_.index.union(events_['t1'].values).drop_duplicates()\n",
    "    px = close.reindex(px, method='bfill')\n",
    "    out = pd.DataFrame(index=events_.index)\n",
    "    out['ret'] = px.loc[events_['t1'].values].values / px.loc[events_.index] - 1\n",
    "    if 'side' in events_:\n",
    "        out['ret'] *= events_['side']\n",
    "    out['bin'] = np.sign(out['ret'])\n",
    "    if 'side' in events_:\n",
    "        out.loc[out['ret'] <= 0, 'bin'] = 0\n",
    "    else:\n",
    "        if t1 is not None:\n",
    "            vertical_first_touch_idx = events_[events_['t1'].isin(t1.values)].index\n",
    "            out.loc[vertical_first_touch_idx, 'bin'] = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_labels(labels: pd.DataFrame, min_pct: float = 0.05) -> pd.DataFrame:\n",
    "    while True:\n",
    "        df0 = labels['bin'].value_counts(normalize=True)\n",
    "        if df0.min() > min_pct or df0.shape[0] < 3:\n",
    "            break\n",
    "        print('dropped label', df0.argmin(), df0.min())\n",
    "        labels = labels[labels['bin'] != df0.index[df0.argmin()]]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Sample Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_conc_events(closeIdx: np.ndarray, t1: pd.Series, molecule: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Computing the number of concurrent events per bar\n",
    "    \n",
    "        Parameters:\n",
    "            closeIdx (np.ndarray): timestamps of close prices\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series with number of labels concurrent at each timestamp\n",
    "    '''\n",
    "    t1 = t1.fillna(closeIdx[-1])\n",
    "    t1 = t1[t1 >= molecule[0]]\n",
    "    t1 = t1.loc[:t1[molecule].max()]\n",
    "    iloc = closeIdx.searchsorted(pd.DatetimeIndex([t1.index[0], t1.max()]))\n",
    "    count = pd.Series([0] * (iloc[1] + 1 - iloc[0]), index=closeIdx[iloc[0]: iloc[1] + 1])\n",
    "    for tIn, tOut in t1.iteritems():\n",
    "        count.loc[tIn: tOut] += 1\n",
    "    return count.loc[molecule[0]: t1[molecule].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_weights(t1: pd.Series, num_conc_events: pd.Series, molecule: np.ndarray) -> pd.Series:\n",
    "    '''\n",
    "    Computing average uniqueness over the event's lifespan\n",
    "    \n",
    "        Parameters:\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            num_conc_events (pd.Series): number of concurrent events per bar\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            weights (pd.Series): weights that represent the average uniqueness\n",
    "    '''\n",
    "    weights = pd.Series([0] * len(molecule), index=molecule)\n",
    "    for tIn, tOut in t1.loc[weights.index].iteritems():\n",
    "        weights.loc[tIn] = (1.0 / num_conc_events.loc[tIn: tOut]).mean()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ind_matrix(barIdx: np.ndarray, t1: pd.Series) -> pd.DataFrame:\n",
    "    '''\n",
    "    Deriving indicator matrix\n",
    "    \n",
    "        Parameters:\n",
    "            barIdx (np.ndarray): indexes of bars\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            \n",
    "        Returns:\n",
    "            indM (pd.DataFrame): binary matrix indicating what bars influence the label for each observation\n",
    "    '''\n",
    "    indM = pd.DataFrame(0, index=barIdx, columns=range(t1.shape[0]))\n",
    "    for i, (t0, t1) in enumerate(t1.iteritems()):\n",
    "        indM.loc[t0:t1, i] = 1.0\n",
    "    return indM\n",
    "\n",
    "\n",
    "def get_avg_uniqueness(indM: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Compute average uniqueness from indicator matrix\n",
    "    '''\n",
    "    c = indM.sum(axis=1)\n",
    "    u = indM.div(c, axis=0)\n",
    "    avg_uniq = u[u > 0].mean()\n",
    "    return avg_uniq\n",
    "\n",
    "\n",
    "def seq_bootstrap(indM: pd.DataFrame, sLength: int = None) -> np.ndarray:\n",
    "    '''\n",
    "    Generate a sample via sequential bootstrap\n",
    "    \n",
    "        Parameters:\n",
    "            indM (pd.DataFrame): binary matrix indicating what bars influence the label for each observation\n",
    "            sLength (int) (optional): sample length (if None, equals number of columns in indM)\n",
    "            \n",
    "        Returns:\n",
    "            phi (np.ndarray): array with indexes of the features sampled by sequential bootstrap\n",
    "    '''\n",
    "    if sLength is None:\n",
    "        sLength = indM.shape[1]\n",
    "    phi = []\n",
    "    while len(phi) < sLength:\n",
    "        avg_uniq = pd.Series()\n",
    "        for i in indM:\n",
    "            indM_ = indM[phi + [i]]\n",
    "            avg_uniq.loc[i] = get_avg_uniqueness(indM_).iloc[-1]\n",
    "        prob = avg_uniq / avg_uniq.sum()\n",
    "        phi += [np.random.choice(indM.columns, p=prob)]\n",
    "    return np.array(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rand_t1(numObs: int, numBars: int, maxH: int) -> pd.Series:\n",
    "    '''\n",
    "    Generate random t1 series\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "        Returns:\n",
    "            t1 (pd.Series)\n",
    "    '''\n",
    "    t1 = pd.Series()\n",
    "    for i in range(numObs):\n",
    "        idx = np.random.randint(0, numBars)\n",
    "        val = idx + np.random.randint(1, maxH)\n",
    "        t1.loc[idx] = val\n",
    "    return t1.sort_index()\n",
    "\n",
    "\n",
    "def aux_MC(numObs: int, numBars: int, maxH: int) -> dict:\n",
    "    '''\n",
    "    Generate random t1 series\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "        Returns:\n",
    "            dict with average uniqueness derived by standard and sequential bootstrap algorithms\n",
    "    '''\n",
    "    t1 = gen_rand_t1(numObs, numBars, maxH)\n",
    "    barIdx = range(t1.max() + 1)\n",
    "    indM = get_ind_matrix(barIdx, t1)\n",
    "    phi = np.random.choice(indM.columns, size=indM.shape[1])\n",
    "    stdU = get_avg_uniqueness(indM[phi]).mean()\n",
    "    phi = seq_bootstrap(indM)\n",
    "    seqU = get_avg_uniqueness(indM[phi]).mean()\n",
    "    return {'stdU': stdU, 'seqU': seqU}\n",
    "\n",
    "\n",
    "def main_MC(numObs: int, numBars: int, maxH: int, numIters: int) -> None:\n",
    "    '''\n",
    "    Run MC simulation for comparing standard and sequential bootstraps\n",
    "    \n",
    "        Parameters:\n",
    "            numObs (int): number of observations for which t1 is generated\n",
    "            numBars (int): number of bars\n",
    "            maxH (int): upper bound for uniform distribution to determine the number of bars spanned by observation\n",
    "            numIters (int): number of MC iterations\n",
    "        Returns:\n",
    "            out (pd.DataFrame): dataframe containing uniqueness obtained by standard and sequential bootstraps\n",
    "    '''\n",
    "    out = pd.DataFrame()\n",
    "    for i in range(numIters):\n",
    "        out = pd.concat((out, pd.DataFrame([aux_MC(numObs, numBars, maxH)])))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_return_weights(\n",
    "    t1: pd.Series, num_conc_events: pd.Series, close: pd.Series, molecule: np.ndarray\n",
    ") -> pd.Series:\n",
    "    '''\n",
    "     Determination of sample weights by absolute return distribution\n",
    "    \n",
    "        Parameters:\n",
    "            t1 (pd.Series): series with the timestamps of the vertical barriers\n",
    "            num_conc_events (pd.Series): number of concurrent events per bar\n",
    "            close (pd.Series): close prices\n",
    "            molecule (np.ndarray): dates of events on which weights are computed\n",
    "            \n",
    "        Returns:\n",
    "            weights (pd.Series): weights that absolute returns\n",
    "    '''\n",
    "    ret = np.log(close).diff()\n",
    "    weights = pd.Series(index=molecule, dtype=object)\n",
    "    for tIn, tOut in t1.loc[weights.index].iteritems():\n",
    "        weights.loc[tIn] = (ret.loc[tIn: tOut] / num_conc_events.loc[tIn: tOut]).sum()\n",
    "    return weights.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_decay(tW: pd.Series, clfLastW: float = 1.0) -> pd.Series:\n",
    "    '''\n",
    "    Apply piecewise-linear decay to observed uniqueness. Newest observation gets weight=1,\n",
    "    oldest observation gets weight=clfLastW.\n",
    "    \n",
    "        Parameters:\n",
    "            tW (pd.Series): observed uniqueness\n",
    "            clfLastW (float): weight for the oldest observation\n",
    "        \n",
    "        Returns:\n",
    "            clfW (pd.Series): series with time-decay factors\n",
    "    '''\n",
    "    clfW = tW.sort_index().cumsum()\n",
    "    if clfLastW >= 0:\n",
    "        slope = (1.0 - clfLastW) / clfW.iloc[-1]\n",
    "    else:\n",
    "        slope = 1. / ((clfLastW + 1) * clfW.iloc[-1])\n",
    "    const = 1.0 - slope * clfW.iloc[-1]\n",
    "    clfW = const + slope * clfW\n",
    "    clfW[clfW < 0] = 0\n",
    "    return clfW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Fractionally Differentiated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(d: float, size: int) -> np.ndarray:\n",
    "    '''\n",
    "    Computing the weights for differentiating the series\n",
    "    \n",
    "        Parameters:\n",
    "            d (float): differentiating factor\n",
    "            size (int): length of weights array\n",
    "            \n",
    "        Returns:\n",
    "            w (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w = [1.0]\n",
    "    for k in range(1, size):\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        w.append(w_)\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w\n",
    "\n",
    "\n",
    "def plot_weights(dRange: list, nPlots: int, size: int) -> None:\n",
    "    '''\n",
    "    Generating plots for weights arrays for different differentiating factors\n",
    "    \n",
    "        Parameters:\n",
    "            dRange (list): list with 2 floats - bounds of the interval\n",
    "            nPlots (int): number of plots\n",
    "            size(int): length of each weights array\n",
    "            \n",
    "        Returns:\n",
    "            weights (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w = pd.DataFrame()\n",
    "    for d in np.linspace(dRange[0], dRange[1], nPlots):\n",
    "        w_ = get_weights(d, size)\n",
    "        w_ = pd.DataFrame(w_, index=range(w_.shape[0])[::-1], columns=[d])\n",
    "        w = w.join(w_, how='outer')\n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    ax.plot(w)\n",
    "    ax.set_xlabel('$k$')\n",
    "    ax.set_ylabel('$w_k$')\n",
    "    ax.legend(np.round(np.linspace(dRange[0], dRange[1], nPlots), 2), loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_diff(series: pd.DataFrame, d: float, thres: float = 0.01) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fractional differentiation with increasing width window\n",
    "    Note 1: For thres=1, nothing is skipped\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]\n",
    "    \n",
    "        Parameters:\n",
    "            series (pd.DataFrame): dataframe with time series\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for skipping some of the first observations\n",
    "        \n",
    "        Returns:\n",
    "            df (pd.DataFrame): dataframe with differentiated series\n",
    "    '''\n",
    "    w = get_weights(d, series.shape[0])\n",
    "    w_ = np.cumsum(abs(w))\n",
    "    w_ /= w_[-1]\n",
    "    skip = w_[w_ > thres].shape[0]\n",
    "    \n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        seriesF, df_ = series[[name]].fillna(method='ffill').dropna(), \\\n",
    "                       pd.Series(index=np.arange(series.shape[0]), dtype=object)\n",
    "        for iloc in range(skip, seriesF.shape[0]):\n",
    "            loc = seriesF.index[iloc]\n",
    "            if not np.isfinite(series.loc[loc, name]):\n",
    "                continue    # exclude NAs\n",
    "            df_[loc] = np.dot(w[-(iloc + 1):, :].T, seriesF.loc[:loc])[0, 0]\n",
    "        df[name] = df_.dropna().copy(deep=True)\n",
    "    df = pd.concat(df, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_ffd(d: float, thres: float) -> np.ndarray:\n",
    "    '''\n",
    "    Computing the weights for differentiating the series with fixed window size\n",
    "    \n",
    "        Parameters:\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for cutting off weights\n",
    "            \n",
    "        Returns:\n",
    "            w (np.ndarray): array contatining weights\n",
    "    '''\n",
    "    w, k = [1.0], 1\n",
    "    while True:\n",
    "        w_ = -w[-1] / k * (d - k + 1)\n",
    "        if abs(w_) < thres:\n",
    "            break\n",
    "        w.append(w_)\n",
    "        k += 1\n",
    "    w = np.array(w[::-1]).reshape(-1, 1)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_diff_ffd(series: pd.DataFrame, d: float, thres: float = 1e-5) -> pd.DataFrame:\n",
    "    '''\n",
    "    Fractional differentiation with constant width window\n",
    "    Note 1: thres determines the cut-off weight for the window\n",
    "    Note 2: d can be any positive fractional, not necessarily bounded [0,1]\n",
    "    \n",
    "        Parameters:\n",
    "            series (pd.DataFrame): dataframe with time series\n",
    "            d (float): differentiating factor\n",
    "            thres (float): threshold for cutting off weights\n",
    "        \n",
    "        Returns:\n",
    "            df (pd.DataFrame): dataframe with differentiated series\n",
    "    '''\n",
    "    w = get_weights_ffd(d, thres)\n",
    "    width = len(w) - 1\n",
    "    \n",
    "    df = {}\n",
    "    for name in series.columns:\n",
    "        seriesF, df_ = series[[name]].fillna(method='ffill').dropna(), \\\n",
    "                       pd.Series(index=np.arange(series.shape[0]), dtype=object)\n",
    "        for iloc1 in range(width, seriesF.shape[0]):\n",
    "            loc0, loc1 = seriesF.index[iloc1 - width], seriesF.index[iloc1]\n",
    "            if not np.isfinite(series.loc[loc1,name]):\n",
    "                continue    # exclude NAs\n",
    "            df_[loc1]=np.dot(w.T,seriesF.loc[loc0:loc1])[0, 0]\n",
    "        df[name] = df_.dropna().copy(deep=True)\n",
    "    df = pd.concat(df, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_min_ffd(process: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "                 apply_constant_width: bool = True, thres: float = 0.01) -> None:\n",
    "    '''\n",
    "    Finding the minimum differentiating factor that passes the ADF test\n",
    "    \n",
    "        Parameters:\n",
    "            process (np.ndarray): array with random process values\n",
    "            apply_constant_width (bool): flag that shows whether to use constant width window (if True)\n",
    "                                         or increasing width window (if False)\n",
    "            thres (float): threshold for cutting off weights\n",
    "    '''\n",
    "    out = pd.DataFrame(columns=['adfStat', 'pVal', 'lags', 'nObs', '95% conf'], dtype=object)\n",
    "    printed = False\n",
    "    \n",
    "    for d in np.linspace(0, 2, 21):\n",
    "        if apply_constant_width:\n",
    "            process_diff = frac_diff_ffd(pd.DataFrame(process), d, thres)\n",
    "        else:\n",
    "            process_diff = frac_diff(pd.DataFrame(process), d, thres)    \n",
    "        test_results = adfuller(process_diff, maxlag=1, regression='c', autolag=None)\n",
    "        out.loc[d] = list(test_results[:4]) + [test_results[4]['5%']]\n",
    "        if test_results[1] <= 0.05 and not printed:\n",
    "            print(f'Minimum d required: {d}')\n",
    "            printed = True\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(11, 7))\n",
    "    ax.plot(out['adfStat'])\n",
    "    ax.axhline(out['95% conf'].mean(), linewidth=1, color='r', linestyle='dotted')\n",
    "    ax.set_title('Searching for minimum $d$')\n",
    "    ax.set_xlabel('$d$')\n",
    "    ax.set_ylabel('ADF statistics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_adf_results(process: np.ndarray) -> None:\n",
    "    '''\n",
    "    Printing the results of the Augmented Dickey–Fuller test\n",
    "    '''\n",
    "    adf, p_value, _, _, _ = adfuller(process, maxlag=1, regression='c', autolag=None)\n",
    "    print(f'ADF statistics: {adf}')\n",
    "    print(f'p-value: {p_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
